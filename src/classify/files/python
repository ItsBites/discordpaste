from copy_state_db import CopyStateDB
from faster_ordered_dict import FasterOrderedDict
import gevent
import gevent.monkey
from gevent.pool import Pool
from pymongo.errors import DuplicateKeyError
from pymongo.read_preferences import ReadPreference
import time
import utils
from utils import auto_retry, log_exceptions, squelch_keyboard_interrupt

log = utils.get_logger(__name__)

INSERT_SIZE = 250
INSERT_POOL_SIZE = 40

#
# Copy collection
#

class Stats(object):
    def __init__(self):
        self.start_time = self.adj_start_time = time.time()
        self.inserted = 0
        self.total_docs = None
        self.duplicates = 0 # not a true count of duplicates; just an exception count
        self.exceptions = 0
        self.retries = 0

    def log(self, adjusted=False):
        start_time = self.adj_start_time if adjusted else self.start_time
        qps = int(float(self.inserted) / (time.time() - start_time))
        pct = int(float(self.inserted)/self.total_docs*100.0)
        log.info("%d%% | %d / %d copied | %d/sec | %d dupes | %d exceptions | %d retries" % 
                 (pct, self.inserted, self.total_docs, qps, self.duplicates,
                  self.exceptions, self.retries))


@auto_retry
def _find_and_insert_batch_worker(source_collection, dest_collection, ids, stats):
    """
    greenlet responsible for copying a set of documents
    """

    # read documents from source
    cursor = source_collection.find({'_id': {'$in': ids}})
    cursor.batch_size(len(ids))
    docs = [doc for doc in cursor]

    # perform copy as a single batch
    ids_inserted = []
    try:
        ids_inserted = dest_collection.insert(docs, continue_on_error=True)
    except DuplicateKeyError:
        # this isn't an exact count, but it's more work than it's worth to get an exact
        # count of duplicate _id's
        stats.duplicates += 1
    stats.inserted += len(ids_inserted)


def _copy_stats_worker(stats):
    """
    Periodically print stats relating to the initial copy.
    """
    while True:
        stats.log()
        gevent.sleep(1)


@log_exceptions
@squelch_keyboard_interrupt
def copy_collection(source, dest, state_path, percent):
    """
    Copies all documents from source to destination collection. Inserts documents in
    batches using insert workers, which are each run in their own greenlet. Ensures that
    the destination is empty before starting the copy.

    Does no safety checks -- this is up to the caller.

    @param source      dict of (host, port, db, collection) for the source
    @param dest        dict of (host, port, db, collection) for the destination
    @param state_path  path of state database
    @param percent     percentage of documents to copy
    """
    gevent.monkey.patch_socket()

    # open state database
    state_db = CopyStateDB(state_path)

    # connect to mongo
    source_client = utils.mongo_connect(source['host'], source['port'],
                                        ensure_direct=True,
                                        max_pool_size=30,
                                        read_preference=ReadPreference.SECONDARY,
                                        document_class=FasterOrderedDict)

    source_collection = source_client[source['db']][source['collection']]
    if source_client.is_mongos:
        raise Exception("for performance reasons, sources must be mongod instances; %s:%d is not",
                        source['host'], source['port'])

    dest_client = utils.mongo_connect(dest['host'], dest['port'],
                                      max_pool_size=30,
                                      document_class=FasterOrderedDict)
    dest_collection = dest_client[dest['db']][dest['collection']]

    # record timestamp of last oplog entry, so that we know where to start applying ops
    # later
    oplog_ts = utils.get_last_oplog_entry(source_client)['ts']
    state_db.update_oplog_ts(source, dest, oplog_ts)

    # for testing copying of indices quickly
    if percent == 0:
        log.info("skipping copy because of --percent 0 parameters")
        state_db.update_state(source, dest, CopyStateDB.STATE_WAITING_FOR_INDICES)
        return

    stats = Stats()
    stats.total_docs = int(source_collection.count())
    if percent:
        # hack-ish but good enough for a testing-only feature
        stats.total_docs = int(stats.total_docs * (float(percent)/100.0))

    # get all _ids, which works around a mongo bug/feature that causes massive slowdowns
    # of long-running, large reads over time
    ids = []
    cursor = source_collection.find(fields=["_id"], snapshot=True, timeout=False)
    cursor.batch_size(5000)
    insert_pool = Pool(INSERT_POOL_SIZE)
    stats_greenlet = gevent.spawn(_copy_stats_worker, stats)
    for doc in cursor:
        _id = doc['_id']

        if percent is not None and not utils.id_in_subset(_id, percent):
            continue

        # when we've gathered enough _ids, spawn a worker greenlet to batch copy the
        # documents corresponding to them
        ids.append(_id)
        if len(ids) % INSERT_SIZE == 0:
            outgoing_ids = ids
            ids = []
            insert_pool.spawn(_find_and_insert_batch_worker,
                              source_collection=source_collection,
                              dest_collection=dest_collection,
                              ids=outgoing_ids,
                              stats=stats)
        gevent.sleep()

    # insert last batch of documents
    if len(ids) > 0:        
        _find_and_insert_batch_worker(source_collection=source_collection,
                                      dest_collection=dest_collection,
                                      ids=ids,
                                      stats=stats)
        stats.log()

    # wait until all other outstanding inserts have finished
    insert_pool.join()
    stats_greenlet.kill()
    log.info("done with initial copy")

    state_db.update_state(source, dest, CopyStateDB.STATE_WAITING_FOR_INDICES)

    # yeah, we potentially leak connections here, but that shouldn't be a big deal


def copy_indexes(source, dest):
    """
    Copies all indexes from source to destination, preserving options such as unique
    and sparse.
    """
    # connect to mongo instances
    source_client = utils.mongo_connect(source['host'], source['port'],
                                        ensure_direct=True,
                                        max_pool_size=1,
                                        read_preference=ReadPreference.SECONDARY)
    source_collection = source_client[source['db']][source['collection']]

    dest_client = utils.mongo_connect(dest['host'], dest['port'], max_pool_size=1)
    dest_collection = dest_client[dest['db']][dest['collection']] 

    # copy indices
    for name, index in source_collection.index_information().items():
        kwargs = { 'name': name }
        index_key = None
        for k, v in index.items():
            if k in ['unique', 'sparse']:
                kwargs[k] = v
            elif k == 'v':
                continue
            elif k == 'key':
                # sometimes, pymongo will give us floating point numbers, so let's make sure
                # they're ints instead
                index_key = [(field, int(direction)) for (field, direction) in v]
            else:
                raise NotImplementedError("don't know how to handle index info key %s" % k)
            # TODO: there are other index options that probably aren't handled here

        assert index_key is not None
        log.info("ensuring index on %s (options = %s)", index_key, kwargs)
        dest_collection.ensure_index(index_key, **kwargs)

		import functools
import gc
import gevent
import logging
import pymongo
from pymongo.errors import AutoReconnect, ConnectionFailure, OperationFailure, TimeoutError
import signal
import sys

loggers = {}
def get_logger(name):
    """
    get a logger object with reasonable defaults for formatting

    @param name used to identify the logger (though not currently useful for anything)
    """
    global loggers
    if name in loggers:
        return loggers[name]

    logger = logging.getLogger(name)
    logger.setLevel(logging.DEBUG)
    sh = logging.StreamHandler()
    sh.setFormatter(logging.Formatter("%(asctime)s [%(levelname)s:%(processName)s] %(message)s",
                                      "%m-%d %H:%M:%S"))
    logger.addHandler(sh)

    loggers[name] = logger
    return logger

log = get_logger("utils")


def mongo_connect(host, port, ensure_direct=False, secondary_only=False, max_pool_size=1,
                  socketTimeoutMS=None, w=0, read_preference=None, document_class=dict,
                  replicaSet=None, slave_okay=None):
    """
    Same as MongoClient.connect, except that we are paranoid and ensure that cursors
    # point to what we intended to point them to. Also sets reasonable defaults for our
    needs.

    @param host            host to connect to
    @param port            port to connect to
    @param ensure_direct   do safety checks to ensure we are connected to specified mongo instance

    most other keyword arguments mirror those for pymongo.MongoClient
    """
    options = dict(
        host=host,
        port=port,
        socketTimeoutMS=socketTimeoutMS,
        use_greenlets=True,
        max_pool_size=max_pool_size,
        w=1,
        document_class=document_class)
    if replicaSet is not None:
        options['replicaSet'] = replicaSet
    if read_preference is not None:
        options['read_preference'] = read_preference
    if slave_okay is not None:
        options['slave_okay'] = slave_okay
    client = pymongo.MongoClient(**options)

    if ensure_direct:
        # make sure we are connected to mongod/mongos that was specified; mongodb drivers
        # have the tendency of doing "magical" things in terms of connecting to other boxes
        test_collection = client['local']['test']
        test_cursor = test_collection.find(slave_okay=True, limit=1)
        connection = test_cursor.collection.database.connection
        if connection.host != host or connection.port != port:
            raise ValueError("connected to %s:%d (expected %s:%d)" %
                             (connection.host, connection.port, host, port))

    return client


def parse_mongo_url(url):
    """
    Takes in pseudo-URL of form

    host[:port]/db/collection (e.g. localhost/prod_maestro/emails)

    and returns a dictionary containing elements 'host', 'port', 'db', 'collection'
    """
    try:
        host, db, collection = url.split('/')
    except ValueError:
        raise ValueError("urls be of format: host[:port]/db/collection")

    host_tokens = host.split(':')
    if len(host_tokens) == 2:
        host = host_tokens[0]
        port = int(host_tokens[1])
    elif len(host_tokens) == 1:
        port = 27017
    elif len(host_tokens) > 2:
        raise ValueError("urls be of format: host[:port]/db/collection")

    return dict(host=host, port=port, db=db, collection=collection)


def _source_file_syntax():
    print "--source files must be of the following format:"
    print "database_name.collection_name"
    print "mongo-shard-1.foo.com"
    print "mongo-shard-2.foo.com:27019"
    print "..."
    sys.exit(1)


def parse_source_file(filename):
    """
    parses an input file passed to the --source parameter as a list of dicts that contain
    these fields:

    host
    port
    db: database name
    collection
    """
    sources = []

    with open(filename, "r") as source_file:
        fullname = source_file.readline().strip()
        try:
            db, collection = fullname.split('.')
        except ValueError:
            _source_file_syntax()

        for source in [line.strip() for line in source_file]:
            tokens = source.split(':')
            if len(tokens) == 1:
                host = tokens[0]
                port = 27017
            elif len(tokens) == 2:
                host, port = tokens
                port = int(port)
            else:
                raise ValueError("%s is not a valid source", source)

            sources.append(dict(host=host, port=port, db=db, collection=collection))

    return sources


def get_last_oplog_entry(client):
    """
    gets most recent oplog entry from the given pymongo.MongoClient
    """
    oplog = client['local']['oplog.rs']
    cursor = oplog.find().sort('$natural', pymongo.DESCENDING).limit(1)
    docs = [doc for doc in cursor]
    if not docs:
        raise ValueError("oplog has no entries!")
    return docs[0]


def tune_gc():
    """
    normally, GC is too aggressive; use kmod's suggestion for tuning it
    """
    gc.set_threshold(25000, 25, 10)


def id_in_subset(_id, pct):
    """
    Returns True if _id fits in our definition of a "subset" of documents.
    Used for testing only.
    """
    return (hash(_id) % 100) < pct


def trim(s, prefixes, suffixes):
    """
    naive function that trims off prefixes and suffixes
    """
    for prefix in prefixes:
        if s.startswith(prefix):
            s = s[len(prefix):]

    for suffix in suffixes:
        if s.endswith(suffix):
            s = s[:-len(suffix)]

    return s


def log_exceptions(func):
    """
    logs exceptions using logger, which includes host:port info
    """
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        if 'stats' in kwargs:
            # stats could be a keyword arg
            stats = kwargs['stats']
        elif len(args) > 0:
            # or the last positional arg
            stats = args[-1]
            if not hasattr(stats, 'exceptions'):
                stats = None
        else:
            # or not...
            stats = None

        try:
            return func(*args, **kwargs)
        except SystemExit:
            # just exit, don't log / catch when we're trying to exit()
            raise
        except:
            log.exception("uncaught exception")
            # increment exception counter if one is available to us
            if stats:
                stats.exceptions += 1
    return wrapper


def squelch_keyboard_interrupt(func):
    """
    suppresses KeyboardInterrupts, to avoid stack trace explosion when pressing Control-C
    """
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except KeyboardInterrupt:
            sys.exit(1)
    return wrapper


def wait_for_processes(processes):
    try:
        [process.join() for process in processes]
    except KeyboardInterrupt:
        # prevent a frustrated user from interrupting our cleanup
        signal.signal(signal.SIGINT, signal.SIG_IGN)

        # if a user presses Control-C, we still need to terminate child processes and join()
        # them, to avoid zombie processes
        for process in processes:
            process.terminate()
            process.join()
        log.error("exiting...")
        sys.exit(1)


def auto_retry(func):
    """
    decorator that automatically retries a MongoDB operation if we get an AutoReconnect
    exception

    do not combine with @log_exceptions!!
    """
    MAX_RETRIES = 20  # yes, this is sometimes needed
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        # try to keep track of # of retries we've had to do
        if 'stats' in kwargs:
            # stats could be a keyword arg
            stats = kwargs['stats']
        elif len(args) > 0:
            # or the last positional arg
            stats = args[-1]
            if not hasattr(stats, 'retries'):
                stats = None
                log.warning("couldn't find stats")
        else:
            # or not...
            stats = None
            log.warning("couldn't find stats")

        failures = 0
        while True:
            try:
                return func(*args, **kwargs)
            except (AutoReconnect, ConnectionFailure, OperationFailure, TimeoutError):
                failures += 1
                if stats:
                    stats.retries += 1
                if failures >= MAX_RETRIES:
                    log.exception("FAILED after %d retries", MAX_RETRIES)
                    if stats:
                        stats.exceptions += 1
                    raise
                gevent.sleep(2 * failures)
                log.exception("retry %d after exception", failures)
    return wrapper

	#!/usr/bin/env python

"""
A simple OAuth implementation for authenticating users with third party
websites.

A typical use case inside an AppEngine controller would be:

1) Create the OAuth client. In this case we'll use the Twitter client,
  but you could write other clients to connect to different services.

  import oauth

  consumer_key = "LKlkj83kaio2fjiudjd9...etc"
  consumer_secret = "58kdujslkfojkjsjsdk...etc"
  callback_url = "http://www.myurl.com/callback/twitter"

  client = oauth.TwitterClient(consumer_key, consumer_secret, callback_url)

2) Send the user to Twitter in order to login:

  self.redirect(client.get_authorization_url())

3) Once the user has arrived back at your callback URL, you'll want to
  get the authenticated user information.

  auth_token = self.request.get("oauth_token")
  auth_verifier = self.request.get("oauth_verifier")
  user_info = client.get_user_info(auth_token, auth_verifier=auth_verifier)

  The "user_info" variable should then contain a dictionary of various
  user information (id, picture url, etc). What you do with that data is up
  to you.

  That's it!

4) If you need to, you can also call other other API URLs using
  client.make_request() as long as you supply a valid API URL and an access
  token and secret. Note, you may need to set method=urlfetch.POST.

@author: Mike Knapp
@copyright: Unrestricted. Feel free to use modify however you see fit. Please
note however this software is unsupported. Please don't email me about it. :)
"""

from google.appengine.api import memcache
from google.appengine.api import urlfetch
from google.appengine.ext import db

from cgi import parse_qs
#ImportError: No module named django.utils
#from django.utils import simplejson as json
import json
from hashlib import sha1
from hmac import new as hmac
from random import getrandbits
from time import time
from urllib import urlencode
from urllib import quote as urlquote
from urllib import unquote as urlunquote

import logging


TWITTER = "twitter"
YAHOO = "yahoo"
MYSPACE = "myspace"
DROPBOX = "dropbox"
LINKEDIN = "linkedin"
YAMMER = "yammer"


class OAuthException(Exception):
  pass


def get_oauth_client(service, key, secret, callback_url):
  """Get OAuth Client.

  A factory that will return the appropriate OAuth client.
  """

  if service == TWITTER:
    return TwitterClient(key, secret, callback_url)
  elif service == YAHOO:
    return YahooClient(key, secret, callback_url)
  elif service == MYSPACE:
    return MySpaceClient(key, secret, callback_url)
  elif service == DROPBOX:
    return DropboxClient(key, secret, callback_url)
  elif service == LINKEDIN:
    return LinkedInClient(key, secret, callback_url)
  elif service == YAMMER:
    return YammerClient(key, secret, callback_url)
  else:
    raise Exception, "Unknown OAuth service %s" % service


class AuthToken(db.Model):
  """Auth Token.

  A temporary auth token that we will use to authenticate a user with a
  third party website. (We need to store the data while the user visits
  the third party website to authenticate themselves.)

  TODO: Implement a cron to clean out old tokens periodically.
  """

  service = db.StringProperty(required=True)
  token = db.StringProperty(required=True)
  secret = db.StringProperty(required=True)
  created = db.DateTimeProperty(auto_now_add=True)


class OAuthClient():

  def __init__(self, service_name, consumer_key, consumer_secret, request_url,
               access_url, callback_url=None):
    """ Constructor."""

    self.service_name = service_name
    self.consumer_key = consumer_key
    self.consumer_secret = consumer_secret
    self.request_url = request_url
    self.access_url = access_url
    self.callback_url = callback_url

  def prepare_request(self, url, token="", secret="", additional_params=None,
                      method=urlfetch.GET, t=None, nonce=None):
    """Prepare Request.

    Prepares an authenticated request to any OAuth protected resource.

    Returns the payload of the request.
    """

    def encode(text):
      return urlquote(str(text), "~")

    params = {
      "oauth_consumer_key": self.consumer_key,
      "oauth_signature_method": "HMAC-SHA1",
      "oauth_timestamp": t if t else str(int(time())),
      "oauth_nonce": nonce if nonce else str(getrandbits(64)),
      "oauth_version": "1.0"
    }

    if token:
      params["oauth_token"] = token
    elif self.callback_url:
      params["oauth_callback"] = self.callback_url

    if additional_params:
        params.update(additional_params)

    for k,v in params.items():
        if isinstance(v, unicode):
            params[k] = v.encode('utf8')

    # Join all of the params together.
    params_str = "&".join(["%s=%s" % (encode(k), encode(params[k]))
                           for k in sorted(params)])

    # Join the entire message together per the OAuth specification.
    message = "&".join(["GET" if method == urlfetch.GET else "POST",
                       encode(url), encode(params_str)])

    # Create a HMAC-SHA1 signature of the message.
    key = "%s&%s" % (self.consumer_secret, secret) # Note compulsory "&".
    signature = hmac(key, message, sha1)
    digest_base64 = signature.digest().encode("base64").strip()
    params["oauth_signature"] = digest_base64

    # Construct the request payload and return it
    return urlencode(params)

  def make_async_request(self, url, token="", secret="", additional_params=None,
                         protected=False, method=urlfetch.GET, headers={}):
    """Make Request.

    Make an authenticated request to any OAuth protected resource.

    If protected is equal to True, the Authorization: OAuth header will be set.

    A urlfetch response object is returned.
    """

    payload = self.prepare_request(url, token, secret, additional_params,
                                   method)

    if method == urlfetch.GET:
      url = "%s?%s" % (url, payload)
      payload = None

    if protected:
      headers["Authorization"] = "OAuth"

    rpc = urlfetch.create_rpc(deadline=10.0)
    urlfetch.make_fetch_call(rpc, url, method=method, headers=headers,
                             payload=payload)
    return rpc

  def make_request(self, url, token="", secret="", additional_params=None,
                   protected=False, method=urlfetch.GET, headers={}):

    return self.make_async_request(url, token, secret, additional_params,
                                   protected, method, headers).get_result()

  def get_authorization_url(self):
    """Get Authorization URL.

    Returns a service specific URL which contains an auth token. The user
    should be redirected to this URL so that they can give consent to be
    logged in.
    """

    raise NotImplementedError, "Must be implemented by a subclass"

  def get_user_info(self, auth_token, auth_verifier=""):
    """Get User Info.

    Exchanges the auth token for an access token and returns a dictionary
    of information about the authenticated user.
    """

    auth_token = urlunquote(auth_token)
    auth_verifier = urlunquote(auth_verifier)

    auth_secret = memcache.get(self._get_memcache_auth_key(auth_token))

    if not auth_secret:
      result = AuthToken.gql("""
        WHERE
          service = :1 AND
          token = :2
        LIMIT
          1
      """, self.service_name, auth_token).get()

      if not result:
        logging.error("The auth token %s was not found in our db" % auth_token)
        raise Exception, "Could not find Auth Token in database"
      else:
        auth_secret = result.secret

    response = self.make_request(self.access_url,
                                 token=auth_token,
                                 secret=auth_secret,
                                 additional_params={"oauth_verifier":
                                                     auth_verifier})

    # Extract the access token/secret from the response.
    result = self._extract_credentials(response)

    # Try to collect some information about this user from the service.
    user_info = self._lookup_user_info(result["token"], result["secret"])
    user_info.update(result)

    return user_info

  def _get_auth_token(self):
    """Get Authorization Token.

    Actually gets the authorization token and secret from the service. The
    token and secret are stored in our database, and the auth token is
    returned.
    """

    response = self.make_request(self.request_url)
    result = self._extract_credentials(response)

    auth_token = result["token"]
    auth_secret = result["secret"]

    # Save the auth token and secret in our database.
    auth = AuthToken(service=self.service_name,
                     token=auth_token,
                     secret=auth_secret)
    auth.put()

    # Add the secret to memcache as well.
    memcache.set(self._get_memcache_auth_key(auth_token), auth_secret,
                 time=20*60)

    return auth_token

  def _get_memcache_auth_key(self, auth_token):

    return "oauth_%s_%s" % (self.service_name, auth_token)

  def _extract_credentials(self, result):
    """Extract Credentials.

    Returns an dictionary containing the token and secret (if present).
    Throws an Exception otherwise.
    """

    token = None
    secret = None
    parsed_results = parse_qs(result.content)

    if "oauth_token" in parsed_results:
      token = parsed_results["oauth_token"][0]

    if "oauth_token_secret" in parsed_results:
      secret = parsed_results["oauth_token_secret"][0]

    if not (token and secret) or result.status_code != 200:
      logging.error("Could not extract token/secret: %s" % result.content)
      raise OAuthException("Problem talking to the service")

    return {
      "service": self.service_name,
      "token": token,
      "secret": secret
    }

  def _lookup_user_info(self, access_token, access_secret):
    """Lookup User Info.

    Complies a dictionary describing the user. The user should be
    authenticated at this point. Each different client should override
    this method.
    """

    raise NotImplementedError, "Must be implemented by a subclass"

  def _get_default_user_info(self):
    """Get Default User Info.

    Returns a blank array that can be used to populate generalized user
    information.
    """

    return {
      "id": "",
      "username": "",
      "name": "",
      "picture": ""
    }


class TwitterClient(OAuthClient):
  """Twitter Client.

  A client for talking to the Twitter API using OAuth as the
  authentication model.
  """

  def __init__(self, consumer_key, consumer_secret, callback_url):
    """Constructor."""

    OAuthClient.__init__(self,
        TWITTER,
        consumer_key,
        consumer_secret,
        "https://api.twitter.com/oauth/request_token",
        "https://api.twitter.com/oauth/access_token",
        callback_url)

  def get_authorization_url(self):
    """Get Authorization URL."""

    token = self._get_auth_token()
    return "https://api.twitter.com/oauth/authorize?oauth_token=%s" % token

  def get_authenticate_url(self):
    """Get Authentication URL."""
    token = self._get_auth_token()
    return "https://api.twitter.com/oauth/authenticate?oauth_token=%s" % token

  def _lookup_user_info(self, access_token, access_secret):
    """Lookup User Info.

    Lookup the user on Twitter.
    """

    response = self.make_request(
        "https://api.twitter.com/1.1/account/verify_credentials.json",
        token=access_token, secret=access_secret, protected=True)

    data = json.loads(response.content)

    user_info = self._get_default_user_info()
    user_info["id"] = data["id"]
    user_info["username"] = data["screen_name"]
    user_info["name"] = data["name"]
    user_info["picture"] = data["profile_image_url"]

    return user_info


class MySpaceClient(OAuthClient):
  """MySpace Client.

  A client for talking to the MySpace API using OAuth as the
  authentication model.
  """

  def __init__(self, consumer_key, consumer_secret, callback_url):
    """Constructor."""

    OAuthClient.__init__(self,
        MYSPACE,
        consumer_key,
        consumer_secret,
        "http://api.myspace.com/request_token",
        "http://api.myspace.com/access_token",
        callback_url)

  def get_authorization_url(self):
    """Get Authorization URL."""

    token = self._get_auth_token()
    return ("http://api.myspace.com/authorize?oauth_token=%s"
            "&oauth_callback=%s" % (token, urlquote(self.callback_url)))

  def _lookup_user_info(self, access_token, access_secret):
    """Lookup User Info.

    Lookup the user on MySpace.
    """

    response = self.make_request("http://api.myspace.com/v1/user.json",
        token=access_token, secret=access_secret, protected=True)

    data = json.loads(response.content)

    user_info = self._get_default_user_info()
    user_info["id"] = data["userId"]
    username = data["webUri"].replace("http://www.myspace.com/", "")
    user_info["username"] = username
    user_info["name"] = data["name"]
    user_info["picture"] = data["image"]

    return user_info


class YahooClient(OAuthClient):
  """Yahoo! Client.

  A client for talking to the Yahoo! API using OAuth as the
  authentication model.
  """

  def __init__(self, consumer_key, consumer_secret, callback_url):
    """Constructor."""

    OAuthClient.__init__(self,
        YAHOO,
        consumer_key,
        consumer_secret,
        "https://api.login.yahoo.com/oauth/v2/get_request_token",
        "https://api.login.yahoo.com/oauth/v2/get_token",
        callback_url)

  def get_authorization_url(self):
    """Get Authorization URL."""

    token = self._get_auth_token()
    return ("https://api.login.yahoo.com/oauth/v2/request_auth?oauth_token=%s"
            % token)

  def _lookup_user_info(self, access_token, access_secret):
    """Lookup User Info.

    Lookup the user on Yahoo!
    """

    user_info = self._get_default_user_info()

    # 1) Obtain the user's GUID.
    response = self.make_request(
        "http://social.yahooapis.com/v1/me/guid", token=access_token,
        secret=access_secret, additional_params={"format": "json"},
        protected=True)

    data = json.loads(response.content)["guid"]
    guid = data["value"]

    # 2) Inspect the user's profile.
    response = self.make_request(
        "http://social.yahooapis.com/v1/user/%s/profile/usercard" % guid,
         token=access_token, secret=access_secret,
         additional_params={"format": "json"}, protected=True)

    data = json.loads(response.content)["profile"]

    user_info["id"] = guid
    user_info["username"] = data["nickname"].lower()
    user_info["name"] = data["nickname"]
    user_info["picture"] = data["image"]["imageUrl"]

    return user_info


class DropboxClient(OAuthClient):
  """Dropbox Client.

  A client for talking to the Dropbox API using OAuth as the authentication
  model.
  """

  def __init__(self, consumer_key, consumer_secret, callback_url):
    """Constructor."""

    OAuthClient.__init__(self,
        DROPBOX,
        consumer_key,
        consumer_secret,
        "https://api.dropbox.com/0/oauth/request_token",
        "https://api.dropbox.com/0/oauth/access_token",
        callback_url)

  def get_authorization_url(self):
    """Get Authorization URL."""

    token = self._get_auth_token()
    return ("http://www.dropbox.com/0/oauth/authorize?"
            "oauth_token=%s&oauth_callback=%s" % (token,
                                                  urlquote(self.callback_url)))

  def _lookup_user_info(self, access_token, access_secret):
    """Lookup User Info.

    Lookup the user on Dropbox.
    """

    response = self.make_request("http://api.dropbox.com/0/account/info",
                                 token=access_token, secret=access_secret,
                                 protected=True)

    data = json.loads(response.content)
    user_info = self._get_default_user_info()
    user_info["id"] = data["uid"]
    user_info["name"] = data["display_name"]
    user_info["country"] = data["country"]

    return user_info


class LinkedInClient(OAuthClient):
  """LinkedIn Client.

  A client for talking to the LinkedIn API using OAuth as the
  authentication model.
  """

  def __init__(self, consumer_key, consumer_secret, callback_url):
    """Constructor."""

    OAuthClient.__init__(self,
        LINKEDIN,
        consumer_key,
        consumer_secret,
        "https://api.linkedin.com/uas/oauth/requestToken",
        "https://api.linkedin.com/uas/oauth/accessToken",
        callback_url)

  def get_authorization_url(self):
    """Get Authorization URL."""

    token = self._get_auth_token()
    return ("https://www.linkedin.com/uas/oauth/authenticate?oauth_token=%s"
            "&oauth_callback=%s" % (token, urlquote(self.callback_url)))

  def _lookup_user_info(self, access_token, access_secret):
    """Lookup User Info.

    Lookup the user on LinkedIn
    """

    user_info = self._get_default_user_info()

    # Grab the user's profile from LinkedIn.
    response = self.make_request("http://api.linkedin.com/v1/people/~:"
                                 "(picture-url,id,first-name,last-name)",
                                 token=access_token,
                                 secret=access_secret,
                                 protected=False,
                                 headers={"x-li-format":"json"})

    data = json.loads(response.content)
    user_info["id"] = data["id"]
    user_info["picture"] = data["pictureUrl"]
    user_info["name"] = data["firstName"] + " " + data["lastName"]
    return user_info


class YammerClient(OAuthClient):
  """Yammer Client.

  A client for talking to the Yammer API using OAuth as the
  authentication model.
  """

  def __init__(self, consumer_key, consumer_secret, callback_url):
    """Constructor."""

    OAuthClient.__init__(self,
        YAMMER,
        consumer_key,
        consumer_secret,
        "https://www.yammer.com/oauth/request_token",
        "https://www.yammer.com/oauth/access_token",
        callback_url)

  def get_authorization_url(self):
    """Get Authorization URL."""

    token = self._get_auth_token()
    return ("https://www.yammer.com/oauth/authorize?oauth_token=%s"
            "&oauth_callback=%s" % (token, urlquote(self.callback_url)))

  def _lookup_user_info(self, access_token, access_secret):
    """Lookup User Info.

    Lookup the user on Yammer
    """

    user_info = self._get_default_user_info()

    # Grab the user's profile from Yammer.
    response = self.make_request("https://www.yammer.com/api/v1/users/current.json",
                                 token=access_token,
                                 secret=access_secret,
                                 protected=False,
                                 headers={"x-li-format":"json"})

    data = json.loads(response.content)
    user_info = self._get_default_user_info()
    user_info["id"] = data["name"]
    user_info["picture"] = data["mugshot_url"]
    user_info["name"] = data["full_name"]
    return user_info

	"""
Twitter OAuth Support for Google App Engine Apps.

Using this in your app should be relatively straightforward:

* Edit the configuration section below with the CONSUMER_KEY and CONSUMER_SECRET
  from Twitter.

* Modify to reflect your App's domain and set the callback URL on Twitter to:

    http://your-app-name.appspot.com/oauth/twitter/callback

* Use the demo in ``MainHandler`` as a starting guide to implementing your app.

Note: You need to be running at least version 1.1.9 of the App Engine SDK.

-- 
I hope you find this useful, tav

"""

# Released into the Public Domain by tav@espians.com

import sys

from datetime import datetime, timedelta
from hashlib import sha1
from hmac import new as hmac
from os.path import dirname, join as join_path
from random import getrandbits
from time import time
from urllib import urlencode, quote as urlquote
from uuid import uuid4
from wsgiref.handlers import CGIHandler

sys.path.insert(0, join_path(dirname(__file__), 'lib')) # extend sys.path

from demjson import decode as decode_json

from google.appengine.api.urlfetch import fetch as urlfetch, GET, POST
from google.appengine.ext import db
from google.appengine.ext.webapp import RequestHandler, WSGIApplication

# ------------------------------------------------------------------------------
# configuration -- SET THESE TO SUIT YOUR APP!!
# ------------------------------------------------------------------------------

OAUTH_APP_SETTINGS = {

    'twitter': {

        'consumer_key': '',
        'consumer_secret': '',

        'request_token_url': 'https://twitter.com/oauth/request_token',
        'access_token_url': 'https://twitter.com/oauth/access_token',
        'user_auth_url': 'http://twitter.com/oauth/authorize',

        'default_api_prefix': 'http://twitter.com',
        'default_api_suffix': '.json',

        },

    'google': {

        'consumer_key': '',
        'consumer_secret': '',

        'request_token_url': 'https://www.google.com/accounts/OAuthGetRequestToken',
        'access_token_url': 'https://www.google.com/accounts/OAuthGetAccessToken',
        'user_auth_url': 'https://www.google.com/accounts/OAuthAuthorizeToken',

        },

    }

CLEANUP_BATCH_SIZE = 100
EXPIRATION_WINDOW = timedelta(seconds=60*60*1) # 1 hour

try:
    from config import OAUTH_APP_SETTINGS
except:
    pass

STATIC_OAUTH_TIMESTAMP = 12345 # a workaround for clock skew/network lag

# ------------------------------------------------------------------------------
# utility functions
# ------------------------------------------------------------------------------

def get_service_key(service, cache={}):
    if service in cache: return cache[service]
    return cache.setdefault(
        service, "%s&" % encode(OAUTH_APP_SETTINGS[service]['consumer_secret'])
        )

def create_uuid():
    return 'id-%s' % uuid4()

def encode(text):
    return urlquote(str(text), '')

def twitter_specifier_handler(client):
    return client.get('/account/verify_credentials')['screen_name']

OAUTH_APP_SETTINGS['twitter']['specifier_handler'] = twitter_specifier_handler

# ------------------------------------------------------------------------------
# db entities
# ------------------------------------------------------------------------------

class OAuthRequestToken(db.Model):
    """OAuth Request Token."""

    service = db.StringProperty()
    oauth_token = db.StringProperty()
    oauth_token_secret = db.StringProperty()
    created = db.DateTimeProperty(auto_now_add=True)

class OAuthAccessToken(db.Model):
    """OAuth Access Token."""

    service = db.StringProperty()
    specifier = db.StringProperty()
    oauth_token = db.StringProperty()
    oauth_token_secret = db.StringProperty()
    created = db.DateTimeProperty(auto_now_add=True)

# ------------------------------------------------------------------------------
# oauth client
# ------------------------------------------------------------------------------

class OAuthClient(object):

    __public__ = ('callback', 'cleanup', 'login', 'logout')

    def __init__(self, service, handler, oauth_callback=None, **request_params):
        self.service = service
        self.service_info = OAUTH_APP_SETTINGS[service]
        self.service_key = None
        self.handler = handler
        self.request_params = request_params
        self.oauth_callback = oauth_callback
        self.token = None

    # public methods

    def get(self, api_method, http_method='GET', expected_status=(200,), **extra_params):

        if not (api_method.startswith('http://') or api_method.startswith('https://')):
            api_method = '%s%s%s' % (
                self.service_info['default_api_prefix'], api_method,
                self.service_info['default_api_suffix']
                )

        if self.token is None:
            self.token = OAuthAccessToken.get_by_key_name(self.get_cookie())

        fetch = urlfetch(self.get_signed_url(
            api_method, self.token, http_method, **extra_params
            ))

        if fetch.status_code not in expected_status:
            raise ValueError(
                "Error calling... Got return status: %i [%r]" %
                (fetch.status_code, fetch.content)
                )

        return decode_json(fetch.content)

    def post(self, api_method, http_method='POST', expected_status=(200,), **extra_params):

        if not (api_method.startswith('http://') or api_method.startswith('https://')):
            api_method = '%s%s%s' % (
                self.service_info['default_api_prefix'], api_method,
                self.service_info['default_api_suffix']
                )

        if self.token is None:
            self.token = OAuthAccessToken.get_by_key_name(self.get_cookie())

        fetch = urlfetch(url=api_method, payload=self.get_signed_body(
            api_method, self.token, http_method, **extra_params
            ), method=http_method)

        if fetch.status_code not in expected_status:
            raise ValueError(
                "Error calling... Got return status: %i [%r]" %
                (fetch.status_code, fetch.content)
                )

        return decode_json(fetch.content)

    def login(self):

        proxy_id = self.get_cookie()

        if proxy_id:
            return "FOO%rFF" % proxy_id
            self.expire_cookie()

        return self.get_request_token()

    def logout(self, return_to='/'):
        self.expire_cookie()
        self.handler.redirect(self.handler.request.get("return_to", return_to))

    # oauth workflow

    def get_request_token(self):

        token_info = self.get_data_from_signed_url(
            self.service_info['request_token_url'], **self.request_params
            )

        token = OAuthRequestToken(
            service=self.service,
            **dict(token.split('=') for token in token_info.split('&'))
            )

        token.put()

        if self.oauth_callback:
            oauth_callback = {'oauth_callback': self.oauth_callback}
        else:
            oauth_callback = {}

        self.handler.redirect(self.get_signed_url(
            self.service_info['user_auth_url'], token, **oauth_callback
            ))

    def callback(self, return_to='/'):

        oauth_token = self.handler.request.get("oauth_token")

        if not oauth_token:
            return get_request_token()

        oauth_token = OAuthRequestToken.all().filter(
            'oauth_token =', oauth_token).filter(
            'service =', self.service).fetch(1)[0]

        token_info = self.get_data_from_signed_url(
            self.service_info['access_token_url'], oauth_token
            )

        key_name = create_uuid()

        self.token = OAuthAccessToken(
            key_name=key_name, service=self.service,
            **dict(token.split('=') for token in token_info.split('&'))
            )

        if 'specifier_handler' in self.service_info:
            specifier = self.token.specifier = self.service_info['specifier_handler'](self)
            old = OAuthAccessToken.all().filter(
                'specifier =', specifier).filter(
                'service =', self.service)
            db.delete(old)

        self.token.put()
        self.set_cookie(key_name)
        self.handler.redirect(return_to)

    def cleanup(self):
        query = OAuthRequestToken.all().filter(
            'created <', datetime.now() - EXPIRATION_WINDOW
            )
        count = query.count(CLEANUP_BATCH_SIZE)
        db.delete(query.fetch(CLEANUP_BATCH_SIZE))
        return "Cleaned %i entries" % count

    # request marshalling

    def get_data_from_signed_url(self, __url, __token=None, __meth='GET', **extra_params):
        return urlfetch(self.get_signed_url(
            __url, __token, __meth, **extra_params
            )).content

    def get_signed_url(self, __url, __token=None, __meth='GET',**extra_params):
        return '%s?%s'%(__url, self.get_signed_body(__url, __token, __meth, **extra_params))

    def get_signed_body(self, __url, __token=None, __meth='GET',**extra_params):

        service_info = self.service_info

        kwargs = {
            'oauth_consumer_key': service_info['consumer_key'],
            'oauth_signature_method': 'HMAC-SHA1',
            'oauth_version': '1.0',
            'oauth_timestamp': int(time()),
            'oauth_nonce': getrandbits(64),
            }

        kwargs.update(extra_params)

        if self.service_key is None:
            self.service_key = get_service_key(self.service)

        if __token is not None:
            kwargs['oauth_token'] = __token.oauth_token
            key = self.service_key + encode(__token.oauth_token_secret)
        else:
            key = self.service_key

        message = '&'.join(map(encode, [
            __meth.upper(), __url, '&'.join(
                '%s=%s' % (encode(k), encode(kwargs[k])) for k in sorted(kwargs)
                )
            ]))

        kwargs['oauth_signature'] = hmac(
            key, message, sha1
            ).digest().encode('base64')[:-1]

        return urlencode(kwargs)

    # who stole the cookie from the cookie jar?

    def get_cookie(self):
        return self.handler.request.cookies.get(
            'oauth.%s' % self.service, ''
            )

    def set_cookie(self, value, path='/'):
        self.handler.response.headers.add_header(
            'Set-Cookie', 
            '%s=%s; path=%s; expires="Fri, 31-Dec-2021 23:59:59 GMT"' %
            ('oauth.%s' % self.service, value, path)
            )

    def expire_cookie(self, path='/'):
        self.handler.response.headers.add_header(
            'Set-Cookie', 
            '%s=; path=%s; expires="Fri, 31-Dec-1999 23:59:59 GMT"' %
            ('oauth.%s' % self.service, path)
            )

# ------------------------------------------------------------------------------
# oauth handler
# ------------------------------------------------------------------------------

class OAuthHandler(RequestHandler):

    def get(self, service, action=''):

        if service not in OAUTH_APP_SETTINGS:
            return self.response.out.write(
                "Unknown OAuth Service Provider: %r" % service
                )

        client = OAuthClient(service, self)

        if action in client.__public__:
            self.response.out.write(getattr(client, action)())
        else:
            self.response.out.write(client.login())

# ------------------------------------------------------------------------------
# modify this demo MainHandler to suit your needs
# ------------------------------------------------------------------------------

HEADER = """
  <html><head><title>Twitter OAuth Demo</title>
  </head><body>
  <h1>Twitter OAuth Demo App</h1>
  """

FOOTER = "</body></html>"

class MainHandler(RequestHandler):
    """Demo Twitter App."""

    def get(self):

        client = OAuthClient('twitter', self)
        gdata = OAuthClient('google', self, scope='http://www.google.com/calendar/feeds')

        write = self.response.out.write; write(HEADER)

        if not client.get_cookie():
            write('<a href="/oauth/twitter/login">Login via Twitter</a>')
            write(FOOTER)
            return

        write('<a href="/oauth/twitter/logout">Logout from Twitter</a><br /><br />')

        info = client.get('/account/verify_credentials')

        write("<strong>Screen Name:</strong> %s<br />" % info['screen_name'])
        write("<strong>Location:</strong> %s<br />" % info['location'])

        rate_info = client.get('/account/rate_limit_status')

        write("<strong>API Rate Limit Status:</strong> %r" % rate_info)

        write(FOOTER)

# ------------------------------------------------------------------------------
# self runner -- gae cached main() function
# ------------------------------------------------------------------------------

def main():

    application = WSGIApplication([
       ('/oauth/(.*)/(.*)', OAuthHandler),
       ('/', MainHandler)
       ], debug=True)

    CGIHandler().run(application)

if __name__ == '__main__':
    main()

	#!/usr/bin/env python

import sys
import ConfigParser
from cells import Game

config = ConfigParser.RawConfigParser()

def get_mind(name):
    full_name = 'minds.' + name
    __import__(full_name)
    mind = sys.modules[full_name]
    mind.name = name
    return mind

bounds = None  # HACK
symmetric = None
mind_list = None

def main():
    global bounds, symmetric, mind_list
    try:
        config.read('tournament.cfg')
        bounds = config.getint('terrain', 'bounds')
        symmetric = config.getboolean('terrain', 'symmetric')
        minds_str = str(config.get('minds', 'minds'))

    except Exception as e:
        print 'Got error: %s' % e
        config.add_section('minds')
        config.set('minds', 'minds', 'mind1,mind2')
        config.add_section('terrain')
        config.set('terrain', 'bounds', '300')
        config.set('terrain', 'symmetric', 'true')

        with open('tournament.cfg', 'wb') as configfile:
            config.write(configfile)

        config.read('tournament.cfg')
        bounds = config.getint('terrain', 'bounds')
        symmetric = config.getboolean('terrain', 'symmetric')
        minds_str = str(config.get('minds', 'minds'))
    mind_list = [(n, get_mind(n)) for n in minds_str.split(',')]

    # accept command line arguments for the minds over those in the config
    try:
        if len(sys.argv)>2:
            mind_list = [(n,get_mind(n)) for n in sys.argv[1:] ]
    except (ImportError, IndexError):
        pass


if __name__ == "__main__":
    main()
    scores = [0 for x in mind_list]
    tournament_list = [[mind_list[a], mind_list[b]] for a in range(len(mind_list)) for b in range (a)]
    for n in range(4):
        for pair in tournament_list:
            game = Game(bounds, pair, symmetric, 5000, headless = True)
            while game.winner == None:
                game.tick()
            if game.winner >= 0:
                idx = mind_list.index(pair[game.winner])
                scores[idx] += 3
            if game.winner == -1:
                idx = mind_list.index(pair[0])
                scores[idx] += 1
                idx = mind_list.index(pair[1])
                scores[idx] += 1
            print scores
            print [m[0] for m in mind_list]
    names = [m[0] for m in mind_list]
    name_score = zip(names,scores)
    f = open("scores.csv",'w')
    srt = sorted(name_score,key=lambda ns: -ns[1])
    for x in srt:
        f.write("%s;%s\n" %(x[0],str(x[1])))
    f.close()

	##############################################################################
#
# Copyright (c) 2006 Zope Corporation and Contributors.
# All Rights Reserved.
#
# This software is subject to the provisions of the Zope Public License,
# Version 2.1 (ZPL).  A copy of the ZPL should accompany this distribution.
# THIS SOFTWARE IS PROVIDED "AS IS" AND ANY AND ALL EXPRESS OR IMPLIED
# WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
# WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND FITNESS
# FOR A PARTICULAR PURPOSE.
#
##############################################################################
"""Bootstrap a buildout-based project

Simply run this script in a directory containing a buildout.cfg.
The script accepts buildout command-line options, so you can
use the -c option to specify an alternate configuration file.

$Id$
"""

import os, shutil, sys, tempfile, urllib2
from optparse import OptionParser

tmpeggs = tempfile.mkdtemp()

is_jython = sys.platform.startswith('java')

# parsing arguments
parser = OptionParser()
parser.add_option("-v", "--version", dest="version",
                          help="use a specific zc.buildout version")
parser.add_option("-d", "--distribute",
                   action="store_true", dest="distribute", default=False,
                   help="Use Disribute rather than Setuptools.")

parser.add_option("-c", None, action="store", dest="config_file",
                   help=("Specify the path to the buildout configuration "
                         "file to be used."))

options, args = parser.parse_args()

# if -c was provided, we push it back into args for buildout' main function
if options.config_file is not None:
    args += ['-c', options.config_file]

if options.version is not None:
    VERSION = '==%s' % options.version
else:
    VERSION = ''

USE_DISTRIBUTE = options.distribute
args = args + ['bootstrap']

to_reload = False
try:
    import pkg_resources
    if not hasattr(pkg_resources, '_distribute'):
        to_reload = True
        raise ImportError
except ImportError:
    ez = {}
    if USE_DISTRIBUTE:
        exec urllib2.urlopen('http://python-distribute.org/distribute_setup.py'
                         ).read() in ez
        ez['use_setuptools'](to_dir=tmpeggs, download_delay=0, no_fake=True)
    else:
        exec urllib2.urlopen('http://peak.telecommunity.com/dist/ez_setup.py'
                             ).read() in ez
        ez['use_setuptools'](to_dir=tmpeggs, download_delay=0)

    if to_reload:
        reload(pkg_resources)
    else:
        import pkg_resources

if sys.platform == 'win32':
    def quote(c):
        if ' ' in c:
            return '"%s"' % c # work around spawn lamosity on windows
        else:
            return c
else:
    def quote (c):
        return c

cmd = 'from setuptools.command.easy_install import main; main()'
ws  = pkg_resources.working_set

if USE_DISTRIBUTE:
    requirement = 'distribute'
else:
    requirement = 'setuptools'

if is_jython:
    import subprocess

    assert subprocess.Popen([sys.executable] + ['-c', quote(cmd), '-mqNxd',
           quote(tmpeggs), 'zc.buildout' + VERSION],
           env=dict(os.environ,
               PYTHONPATH=
               ws.find(pkg_resources.Requirement.parse(requirement)).location
               ),
           ).wait() == 0

else:
    assert os.spawnle(
        os.P_WAIT, sys.executable, quote (sys.executable),
        '-c', quote (cmd), '-mqNxd', quote (tmpeggs), 'zc.buildout' + VERSION,
        dict(os.environ,
            PYTHONPATH=
            ws.find(pkg_resources.Requirement.parse(requirement)).location
            ),
        ) == 0

ws.add_entry(tmpeggs)
ws.require('zc.buildout' + VERSION)
import zc.buildout.buildout
zc.buildout.buildout.main(args)
shutil.rmtree(tmpeggs)

import sys
import random

ans = True

while ans:
    question = raw_input("Ask the magic 8 ball a question: (press enter to quit) ")
    
    answers = random.randint(1,8)
    
    if question == "":
        sys.exit()
    
    elif answers == 1:
        print "It is certain"
    
    elif answers == 2:
        print "Outlook good"
    
    elif answers == 3:
        print "You may rely on it"
    
    elif answers == 4:
        print "Ask again later"
    
    elif answers == 5:
        print "Concentrate and ask again"
    
    elif answers == 6:
        print "Reply hazy, try again"
    
    elif answers == 7:
        print "My reply is no"
    
    elif answers == 8:
        print "My sources say no"

from commandr import command, Run, CommandrUsageError, wraps

@command('greet')
def SayGreeting(name, title='Mr.', times=1, comma=False, caps_lock=False):
  """Greet someone.
  Arguments:
    name - Name to greet.
    title - Title of the person to greet.
    times - Number of time to say the greeting.
    comma - Whether to add a comma after the greeting.
    caps_lock - Whether to output in ALL CAPS.
  """
  message = 'Hi%s %s %s!' % (',' if comma else '', title, name)
  if caps_lock:
    message = message.upper()

  for _ in xrange(times):
    print message

@command
def simple_greet(name):
  """An example of @command without arguments and printing Usage.
  Arguments:
    name - Name to greet.
  """
  if name == 'John':
    raise CommandrUsageError("John is not a valid name.")
  print 'Hi %s!' % name

@command()
def another_simple_greet(name):
  """An example of @command() without arguments.
  Arguments:
    name - Name to greet.
  """
  print 'Aloha %s!' % name

def some_decorator(fn):
  @wraps(fn)
  def _wrapper(*args, **kwargs):
    print 'Wrapper Here!'
    return fn(*args, **kwargs)
  return _wrapper

@command('test_decorated')
@some_decorator
def DecoratedFunction(arg1, arg2=1):
    """An example usage of stacked decorators."""
    print arg1, arg2

if __name__ == '__main__':
  Run(hyphenate=True)

  from distutils.core import setup

setup(name='google_analytics',
      version='0.2',
      description='A simple Django application to integrate Google Analytics into your projects',
      author='Clint Ecker',
      author_email='me@clintecker.com',
      url='http://github.com/clintecker/django-google-analytics/tree/master',
      packages=['google_analytics','google_analytics.templatetags',],
      package_data={'google_analytics': ['templates/google_analytics/*.html']},
      classifiers=['Development Status :: 4 - Beta',
                   'Environment :: Web Environment',
                   'Intended Audience :: Developers',
                   'License :: OSI Approved :: BSD License',
                   'Operating System :: OS Independent',
                   'Programming Language :: Python',
                   'Topic :: Utilities'],
      )

      import os, sys

if __name__ == '__main__':
    os.environ.setdefault('DJANGO_SETTINGS_MODULE', '{{ project_name }}.settings')

    from django.core.management import execute_from_command_line
    execute_from_command_line(sys.argv)

    #!/usr/bin/env python3

from http.server import HTTPServer, BaseHTTPRequestHandler
import cgi
import glob
import json
import mimetypes
import os
import os.path
from urllib.parse import urlparse

# Various config settings for the python server
SETTINGS = {
    'port':        8080,
    'logging':     False,

    'api-save':    '/lib/weltmeister/api/save.php',
    'api-browse':  '/lib/weltmeister/api/browse.php',
    'api-glob':    '/lib/weltmeister/api/glob.php',

    'image-types': ['.png', '.jpg', '.gif', '.jpeg'],

    'mimetypes': {
        'ogg': 'audio/ogg'
    }
}

# Override port if we are on a Heroku server
if os.environ.get('PORT'):
    SETTINGS['port'] = int(os.environ.get('PORT'))

# Get the current directory
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
if BASE_DIR[-1] != '/':
    BASE_DIR += '/'

# Blank favicon - prevents silly 404s from occuring if no favicon is supplied
FAVICON_GIF = 'GIF89a\x01\x00\x01\x00\xf0\x00\x00\xff\xff\xff\x00\x00\x00!\xff\x0bXMP DataXMP\x02?x\x00!\xf9\x04\x05\x00\x00\x00\x00,\x00\x00\x00\x00\x01\x00\x01\x00@\x02\x02D\x01\x00;'

class HTTPHandler(BaseHTTPRequestHandler):

    def send_json(self, obj, code=200, headers=None):
        'Send response as JSON'
        if not headers:
            headers = {}
        headers['Content-Type'] = 'application/json'
        self.send_response(json.dumps(obj), code, headers)

    def send_response(self, mesg, code=200, headers=None):
        'Wraps sending a response down'
        if not headers:
            headers = {}
        if 'Content-Type' not in headers:
            headers['Content-Type'] = 'text/html'
        BaseHTTPRequestHandler.send_response(self, code)
        self.send_header('Content-Length', len(mesg))
        if headers:
            for k, v in headers.items():
                self.send_header(k, v)
        self.end_headers()
        self.wfile.write(mesg)

    def log_request(self, *args, **kwargs):
        'If logging is disabled '
        if SETTINGS['logging']:
            BaseHTTPRequestHandler.log_request(self, *args, **kwargs)

    def init_request(self):
        parts = self.path.split('?', 1)
        self.post_params = {}
        if len(parts) == 1:
            self.file_path = parts[0]
            self.query_params = {}
        else:
            self.file_path = parts[0]
            self.query_params = urlparse.parse_qs(parts[1])

    def do_GET(self):
        self.init_request()
        self.route_request('GET')

    def do_HEAD(self):
        self.init_request()
        self.send_response('')

    def do_POST(self):
        self.init_request()

        # From http://stackoverflow.com/questions/4233218/python-basehttprequesthandler-post-variables
        ctype, pdict = cgi.parse_header(self.headers.getheader('content-type'))
        if ctype == 'multipart/form-data':
            self.post_params = cgi.parse_multipart(self.rfile, pdict)
        elif ctype == 'application/x-www-form-urlencoded':
            length = int(self.headers.getheader('content-length'))
            self.post_params = cgi.parse_qs(self.rfile.read(length), keep_blank_values=1)

        self.route_request('POST')

    def route_request(self, method='GET'):
        if self.file_path == SETTINGS['api-save']:
            self.save()
        elif self.file_path == SETTINGS['api-browse']:
            self.browse()
        elif self.file_path == SETTINGS['api-glob']:
            self.glob()
        elif method == 'GET':
            self.serve_file()
        else:
            self.barf()

    def save(self):
        resp = {'error': 0}
        if 'path' in self.post_params and 'data' in self.post_params:
            path = self.post_params['path'][0]
            path = os.path.join(BASE_DIR, path.replace('..', ''))
            data = self.post_params['data'][0]

            if path.endswith('.js'):
                try:
                    open(path, 'w').write(data)
                except:
                    resp['error'] = 2
                    resp['msg'] = 'Couldn\'t write to file %d' % path

            else:
                resp['error'] = 3
                resp['msg'] = 'File must have a .js suffix'

        else:
            resp['error'] = 1
            resp['msg'] = 'No Data or Path specified'

        return self.send_json(resp)

    def browse(self):
        # Get the directory to scan
        dir = ''
        if 'dir' in self.query_params:
            dir = self.query_params['dir'][0].replace('..', '')
            if dir[-1] != '/':
                dir += '/'

        # Get the dir and files
        dirs = [os.path.join(dir, d) for d in os.listdir(os.path.join(BASE_DIR, dir))
                if os.path.isdir(os.path.join(dir, d))]

        files = glob.glob(dir + '*.*')

        # Filter on file types
        if 'type' in self.query_params:
            types = self.query_params['type']
            if 'images' in types:
                files = [f for f in files if os.path.splitext(f)[1] in SETTINGS['image-types']]
            elif 'scripts' in types:
                files = [f for f in files if os.path.splitext(f)[1] == '.js']

        if os.name == 'nt':
            files = [f.replace('\\', '/') for f in files]
            dirs = [d.replace('\\', '/') for d in dirs]

        response = {
            'files': files,
            'dirs': dirs,
            'parent': False if dir == './' else os.path.dirname(os.path.dirname(dir))
        }
        return self.send_json(response)

    def glob(self):
        globs = self.query_params['glob[]']
        files = []

        for g in globs:
            g = g.replace('..', '')
            more = glob.glob(g)
            files.extend(more)

        if os.name == 'nt':
            files = [f.replace('\\', '/') for f in files]

        return self.send_json(files)

    def guess_type(self, path):
        type, _ = mimetypes.guess_type(path)

        if not type:
            ext = path.split('.')[-1]
            if ext in SETTINGS['mimetypes'].keys():
                type = SETTINGS['mimetypes'][ext]

        # Winblows hack
        if os.name == "nt" and type.startswith("image"):
            type = type.replace("x-", "")

        return type

    def serve_file(self):
        path = self.file_path
        if path == '/':
            path = 'index.html'
        elif path == '/editor':
            path = 'weltmeister.html'

        # Remove the leading forward slash
        if path[0] == '/':
            path = path[1:]

        # Security, remove the ..
        path = path.replace('..', '')

        # Determine the fullpath
        path = os.path.join(BASE_DIR, path)

        try:
            data = open(path, 'rb').read()
            type = self.guess_type(path)
            self.send_response(data, 200, headers={'Content-Type': type})
        except:
            if '/favicon.ico' in path:
                self.send_response(FAVICON_GIF, 200, headers={'Content-Type': 'image/gif'})
            else:
                self.send_response('', 404)

    def barf(self):
        self.send_response('barf', 405)


def main():
    addr = ('', SETTINGS['port'])
    server = HTTPServer(addr, HTTPHandler)
    print('Running ImpactJS Server\nGame:   http://localhost:%d\nEditor: http://localhost:%d/editor' % (addr[1], addr[1]))
    server.serve_forever()

if __name__ == '__main__':
    main()

    #!/usr/bin/env python

import BaseHTTPServer
import cgi
import glob
import json
import mimetypes
import os
import os.path
import urlparse

# Various config settings for the python server
SETTINGS = {
    'port':        8080,
    'logging':     False,

    'api-save':    '/lib/weltmeister/api/save.php',
    'api-browse':  '/lib/weltmeister/api/browse.php',
    'api-glob':    '/lib/weltmeister/api/glob.php',

    'image-types': ['.png', '.jpg', '.gif', '.jpeg'],

    'mimetypes': {
        'ogg': 'audio/ogg'
    }
}

# Override port if we are on a Heroku server
if os.environ.get('PORT'):
    SETTINGS['port'] = int(os.environ.get('PORT'))

# Get the current directory
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
if BASE_DIR[-1] != '/':
    BASE_DIR += '/'

# Blank favicon - prevents silly 404s from occuring if no favicon is supplied
FAVICON_GIF = 'GIF89a\x01\x00\x01\x00\xf0\x00\x00\xff\xff\xff\x00\x00\x00!\xff\x0bXMP DataXMP\x02?x\x00!\xf9\x04\x05\x00\x00\x00\x00,\x00\x00\x00\x00\x01\x00\x01\x00@\x02\x02D\x01\x00;'

class HTTPHandler(BaseHTTPServer.BaseHTTPRequestHandler):

    def send_json(self, obj, code=200, headers=None):
        'Send response as JSON'
        if not headers:
            headers = {}
        headers['Content-Type'] = 'application/json'
        self.send_response(json.dumps(obj), code, headers)

    def send_response(self, mesg, code=200, headers=None):
        'Wraps sending a response down'
        if not headers:
            headers = {}
        if 'Content-Type' not in headers:
            headers['Content-Type'] = 'text/html'
        BaseHTTPServer.BaseHTTPRequestHandler.send_response(self, code)
        self.send_header('Content-Length', len(mesg))
        if headers:
            for k, v in headers.iteritems():
                self.send_header(k, v)
        self.end_headers()
        self.wfile.write(mesg)

    def log_request(self, *args, **kwargs):
        'If logging is disabled '
        if SETTINGS['logging']:
            BaseHTTPServer.BaseHTTPRequestHandler.log_request(self, *args, **kwargs)

    def init_request(self):
        parts = self.path.split('?', 1)
        self.post_params = {}
        if len(parts) == 1:
            self.file_path = parts[0]
            self.query_params = {}
        else:
            self.file_path = parts[0]
            self.query_params = urlparse.parse_qs(parts[1])

    def do_GET(self):
        self.init_request()
        self.route_request('GET')

    def do_HEAD(self):
        self.init_request()
        self.send_response('')

    def do_POST(self):
        self.init_request()

        # From http://stackoverflow.com/questions/4233218/python-basehttprequesthandler-post-variables
        ctype, pdict = cgi.parse_header(self.headers.getheader('content-type'))
        if ctype == 'multipart/form-data':
            self.post_params = cgi.parse_multipart(self.rfile, pdict)
        elif ctype == 'application/x-www-form-urlencoded':
            length = int(self.headers.getheader('content-length'))
            self.post_params = cgi.parse_qs(self.rfile.read(length), keep_blank_values=1)

        self.route_request('POST')

    def route_request(self, method='GET'):
        if self.file_path == SETTINGS['api-save']:
            self.save()
        elif self.file_path == SETTINGS['api-browse']:
            self.browse()
        elif self.file_path == SETTINGS['api-glob']:
            self.glob()
        elif method == 'GET':
            self.serve_file()
        else:
            self.barf()

    def save(self):
        resp = {'error': 0}
        if 'path' in self.post_params and 'data' in self.post_params:
            path = self.post_params['path'][0]
            path = os.path.join(BASE_DIR, path.replace('..', ''))
            data = self.post_params['data'][0]

            if path.endswith('.js'):
                try:
                    open(path, 'w').write(data)
                except:
                    resp['error'] = 2
                    resp['msg'] = 'Couldn\'t write to file %d' % path

            else:
                resp['error'] = 3
                resp['msg'] = 'File must have a .js suffix'

        else:
            resp['error'] = 1
            resp['msg'] = 'No Data or Path specified'

        return self.send_json(resp)

    def browse(self):
        # Get the directory to scan
        dir = ''
        if 'dir' in self.query_params:
            dir = self.query_params['dir'][0].replace('..', '')
            if dir[-1] != '/':
                dir += '/'

        # Get the dir and files
        dirs = [os.path.join(dir, d) for d in os.listdir(os.path.join(BASE_DIR, dir))
                if os.path.isdir(os.path.join(dir, d))]

        files = glob.glob(dir + '*.*')

        # Filter on file types
        if 'type' in self.query_params:
            types = self.query_params['type']
            if 'images' in types:
                files = [f for f in files if os.path.splitext(f)[1] in SETTINGS['image-types']]
            elif 'scripts' in types:
                files = [f for f in files if os.path.splitext(f)[1] == '.js']

        if os.name == 'nt':
            files = [f.replace('\\', '/') for f in files]
            dirs = [d.replace('\\', '/') for d in dirs]

        response = {
            'files': files,
            'dirs': dirs,
            'parent': False if dir == './' else os.path.dirname(os.path.dirname(dir))
        }
        return self.send_json(response)

    def glob(self):
        globs = self.query_params['glob[]']
        files = []

        for g in globs:
            g = g.replace('..', '')
            more = glob.glob(g)
            files.extend(more)

        if os.name == 'nt':
            files = [f.replace('\\', '/') for f in files]

        return self.send_json(files)

    def guess_type(self, path):
        type, _ = mimetypes.guess_type(path)

        if not type:
            ext = path.split('.')[-1]
            if ext in SETTINGS['mimetypes'].keys():
                type = SETTINGS['mimetypes'][ext]

        # Winblows hack
        if os.name == "nt" and type.startswith("image"):
            type = type.replace("x-", "")

        return type

    def serve_file(self):
        path = self.file_path
        if path == '/':
            path = 'index.html'
        elif path == '/editor':
            path = 'weltmeister.html'

        # Remove the leading forward slash
        if path[0] == '/':
            path = path[1:]

        # Security, remove the ..
        path = path.replace('..', '')

        # Determine the fullpath
        path = os.path.join(BASE_DIR, path)

        try:
            data = open(path, 'rb').read()
            type = self.guess_type(path)
            self.send_response(data, 200, headers={'Content-Type': type})
        except:
            if '/favicon.ico' in path:
                self.send_response(FAVICON_GIF, 200, headers={'Content-Type': 'image/gif'})
            else:
                self.send_response('', 404)

    def barf(self):
        self.send_response('barf', 405)


def main():
    addr = ('', SETTINGS['port'])
    server = BaseHTTPServer.HTTPServer(addr, HTTPHandler)
    print 'Running ImpactJS Server\nGame:   http://localhost:%d\nEditor: http://localhost:%d/editor' % (addr[1], addr[1])
    server.serve_forever()

if __name__ == '__main__':
    main()

import os

from setuptools import setup, find_packages

here = os.path.abspath(os.path.dirname(__file__))


def _get_meta_var(name, data, callback_handler=None):
    import re
    matches = re.compile(r'(?:%s)\s*=\s*(.*)' % name).search(data)
    if matches:
        if not callable(callback_handler):
            callback_handler = lambda v: v

        return callback_handler(eval(matches.groups()[0]))

_meta = open(os.path.join(here, 'riakkit', '__init__.py'), 'rb')
_metadata = _meta.read()
_meta.close()

callback = lambda V: ('.'.join(map(str, V[:3])) + '.'.join(V[3:]))
__version__ = _get_meta_var('VERSION', _metadata, callback)
__author__ = _get_meta_var('__author__', _metadata)
__url__ = _get_meta_var('__url__', _metadata)

setup(
    name="riakkit",
    version=__version__,
    author=__author__,
    author_email="shuhao@shuhaowu.com",
    description=("An object mapper for Riak similar to mongokit and couchdbkit"
                 " for Riak"),
    license="LGPL",
    keywords="riak object mapper riakkit database orm",
    url=__url__,
    packages=find_packages(),
    install_requires=['riak'],
    classifiers=[
        "Development Status :: 3 - Alpha",
        "License :: OSI Approved :: GNU Library or Lesser General Public License (LGPL)",
        "Intended Audience :: Developers",
        "Topic :: Database",
        "Topic :: Software Development :: Libraries :: Python Modules"
    ]
)

"""This is the top level riakkit module. It provides a shortcut to the package
contents in a convinient fashion.
It imports everything from under commons.properties as well as
commons.exceptions It also import SimpleDocument, BaseDocument, and Document.
This also sets up EmDocument"""

from riakkit.simple import SimpleDocument, BaseDocument
EmDocument = BaseDocument
from riakkit.document import Document, getClassGivenBucketName
from riakkit.commons.properties import *
from riakkit.commons.exceptions import *


#PEP 386 versioning
VERSION = (0, 6, 0, 'a')
__version__ = ('.'.join(map(str, VERSION[:3])) + '.'.join(VERSION[3:]))
__author__ = "Shuhao Wu"
__url__ = "https://github.com/shuhaowu/riakkit"

from copy import copy
from weakref import WeakValueDictionary

from riakkit.simple.basedocument import BaseDocumentMetaclass, BaseDocument, SimpleDocument
from riakkit.commons.properties import BaseProperty, MultiReferenceProperty, ReferenceProperty
from riakkit.commons import uuid1Key, getUniqueListGivenBucketName, getProperty, walkParents
from riakkit.queries import *
from riakkit.commons.exceptions import *

from riak import RiakObject
from riak.mapreduce import RiakLink

_document_classes = {}

def getClassGivenBucketName(bucket_name):
  """Gets the class associated with a bucket name.
  Args:
    bucket_name: The bucket name. String
  Returns:
    A document subclass associated with that bucket name
  Raises:
    KeyError if bucket_name is not used.
  """
  return _document_classes[bucket_name]


class DocumentMetaclass(BaseDocumentMetaclass):
  """Meta class that the Document class is made from.
  Checks for bucket_name in each class, as those are necessary.
  """

  def __new__(cls, clsname, parents, attrs):
    if clsname == "Document":
      return type.__new__(cls, clsname, parents, attrs)

    client = getProperty("client", attrs, parents)
    if client is None:
      return type.__new__(cls, clsname, parents, attrs)

    meta = {}
    uniques = []
    references_col_classes = []
    references = []

    for name in attrs.keys():
      if isinstance(attrs[name], BaseProperty):
        meta[name] = prop = attrs.pop(name)
        refcls = getattr(prop, "reference_class", False)
        prop.name = name
        if refcls and not issubclass(refcls, Document):
          raise TypeError("ReferenceProperties for Document must be another Document!")

        colname = getattr(prop, "collection_name", False)
        if colname:
          if colname in prop.reference_class._meta:
            raise RiakkitError("%s already in %s!" % (colname, prop.reference_class))
          references_col_classes.append((colname, prop.reference_class, name))
          references.append(name)
        elif prop.unique: # Unique is not allowed with anything that has backref
          prop.unique_bucket = client.bucket(getUniqueListGivenBucketName(attrs["bucket_name"], name))
          uniques.append(name)

    all_parents = reversed(walkParents(parents))
    for p_cls in all_parents:
      meta.update(p_cls._meta)
      uniques.extend(p_cls._uniques)

    attrs["_meta"] = meta
    attrs["_uniques"] = uniques

    # I know why you're here. It took you 1938402 years to finally get here and
    # you want to know what .instances does. Before you vencture onto the next
    # line of code, I would like to take this opportunity to say few words:
    # Am I proud of what I wrote? No. In fact, I'm ashamed of it. I admit, this
    # is a piece of shit. HOWEVER, in my defense, this got it to work, even
    # though it caused a shittonne of other problems. I remember very clearly
    # the time I came up with this "clever" solution. It fixed all my issues.

    # The following line made me so annoyed with this library that a new one is
    # written. You may not trust me anymore after the next line... in fact, I
    # don't even trust myself... but riakkit-ng is probably going to be better.

    attrs["instances"] = WeakValueDictionary()
    attrs["_references"] = references

    new_class = type.__new__(cls, clsname, parents, attrs)

    bucket_name = attrs.get("bucket_name", None)

    new_class.buckets = {}

    if bucket_name is not None:
      if isinstance(bucket_name, basestring):
        new_class.bucket_name = bucket_name = [bucket_name]

      for bn in bucket_name:
        if bn in _document_classes:
          raise RiakkitError("Bucket name of %s already exists in the registry!"
                                % bn)
        else:
          _document_classes[bn] = new_class

        new_class.buckets[bn] = client.bucket(bn)

      if len(new_class.buckets) == 1:
        new_class.bucket = new_class.buckets.values()[0]
      else:
        new_class.bucket = new_class.buckets[bucket_name[0]]

    for colname, rcls, back_name in references_col_classes:
      rcls._meta[colname] = MultiReferenceProperty(reference_class=new_class)
      rcls._meta[colname].name = colname
      rcls._meta[colname].is_reference_back = back_name
      rcls._references.append(colname)

    return new_class

class Document(SimpleDocument):
  """The base Document class for other classes to extend from.
  There are a couple of class variables that needs to be filled out. First is
  client. client is an instance of a RiakClient. The other is bucket_name. This
  is the name of the bucket to be stored in Riak. It must not be shared with
  another Document subclass. Lastly, you may set the  to True or False.
  bucket_name maybe a list of bucket names (strings). This allows for multiple
  buckets for each document. When not specified with each save and get
  operation, the default bucket is used, which is the first item in the list
  of bucket names.
  Class variables that's an instance of the BaseType will be the schema of the
  document.
  """

  __metaclass__ = DocumentMetaclass
  _clsType = 2

  def __init__(self, key=uuid1Key, saved=False, **kwargs):
    """Creates a new document from a bunch of keyword arguments.
    Args:
      key: A string/unicode key or a function that returns a string/unicode key.
           The function takes in 1 argument, and that argument is the kwargs
           that's passed in. Defaults to a lambda function that returns
           uuid1().hex
      saved: Is this object already saved? True or False
      kwargs: Keyword arguments that will fill up the object with data.
    """
    if callable(key):
      key = key(kwargs)

    if not isinstance(key, basestring):
      raise KeyError("%s is not a proper key!" % key)

    if key in self.__class__.instances:
      raise KeyError("%s already exists! Use get instead!" % key)

    self.__dict__["key"] = key

    self._obj = self.bucket.get(self.key) if saved else None
    self._links = set()
    self._indexes = {}

    BaseDocument.__init__(self, **kwargs)

    self.__class__.instances[self.key] = self


  # Oh no... you're here. I would advice you not to read the following and just
  # do something useful.
  # Are you still prepared to understand what's happening?
  # Yes?... Alright then...
  # I apologize for the following:
  def save(self, w=None, dw=None, endpoint=False, bucket=None):
    """Saves the document into the database.
    This will save the object to the database. All linked objects will be saved
    as well.
    Args:
      w: W value
      dw: DW value
      endpoint: See if this is an endpoint. i.e. It will save the documents
                that's modified while modifying this one. Default: False
      bucket: Save to a specific bucket. Default is the default bucket. Only
              has an effect if the document is new.
    """
    dataToBeSaved = self.serialize()
    uniquesToBeDeleted = []
    othersToBeSaved = []

    # Process uniques
    for name in self._uniques:
      if self._data.get(name, None) is None:
        if self._obj: # TODO: could be somehow refactored, as this condition is always true?
          originalValue = self._obj.get_data().get(name, None)
          if originalValue is not None:
            uniquesToBeDeleted.append((self._meta[name].unique_bucket, originalValue))
      else:
        changed = False
        if self._obj:
          originalValue = self._obj.get_data().get(name, None)
          if self._data[name] != originalValue and originalValue is not None:
            uniquesToBeDeleted.append((self._meta[name].unique_bucket, originalValue))
            changed = True
        else:
          changed = True

        if changed and self._meta[name].unique_bucket.get(dataToBeSaved[name]).exists():
          raise IntegrityError(
            field=name,
            message="'%s' already exists for '%s'!" % (self._data[name], name)
          )

    # Process references
    for name in self._references:
      currentDocsKeys = None
      strict = self._meta[name].strict
      colname = self._meta[name].collection_name

      if colname:
        currentDocsKeys = set()
        if isinstance(self._meta[name], ReferenceProperty):
          docs = [getattr(self, name)]
        else:
          docs = getattr(self, name)

        for doc in docs: # These are foreign documents
          if doc is None or (not strict and not doc.__class__.exists(doc.key)):
            continue

          currentDocsKeys.add(doc.key)

          currentList = getattr(doc, colname, [])
          found = False # Linear search algorithm. Maybe binary search??
          for d in currentList:
            if d.key == self.key:
              found = True
              break
          if not found:
            currentList.append(self)
            doc._data[colname] = currentList
            othersToBeSaved.append((doc, False))


      colname = colname or self._meta[name].is_reference_back

      if colname:
        if self._obj:
          originalValues = self._obj.get_data().get(name, [])
          if not isinstance(originalValues, list):
            originalValues = [originalValues]
        else:
          originalValues = []

        if currentDocsKeys is None:
          currentDocsKeys = set()
          for d in self._data[name]:
            if d is None:
              continue
            else:
              currentDocsKeys.add(getattr(d, "key", d))

        for dockey in originalValues:
          if dockey is None:
            continue

          # This means that this specific document is not in the current version,
          # but last version. Hence it needs to be cleaned from the last version.
          if dockey not in currentDocsKeys:
            try:
              doc = self._meta[name].reference_class.load(dockey, True)
            except NotFoundError: # TODO: Another hackjob? This is _probably_ due to we're back deleting the reference.
              continue
            if doc._meta[colname].deleteReference(doc, self):
              othersToBeSaved.append((doc, True)) # CODE-REVIEW: For some reason i feel this won't work for some cases.


    if self._obj:
      self._obj.set_data(dataToBeSaved)
    else:
      bucket = self.buckets.get(bucket, self.bucket)
      self._obj = bucket.new(self.key, dataToBeSaved)

    self._obj.set_links(self.links(True), True)
    self._obj.set_indexes(self.indexes())
    self.key = self._obj.get_key()

    self._obj.store(w=w, dw=dw)
    for name in self._uniques:
      if self._data[name] and not self._meta[name].unique_bucket.get(self._data[name]).exists():
        obj = self._meta[name].unique_bucket.new(self._data[name], {"key" : self.key})
        obj.store(w=w, dw=dw)

    for bucket, key in uniquesToBeDeleted:
      bucket.get(key).delete()

    self.saved = True
    self.deleted = False

    if not endpoint: # CODE-REVIEW: Total hackjob. This gotta be redone
      for doc, end in othersToBeSaved:
        doc.save(w, dw, end)

    return self

  def reload(self, r=None, vtag=None):
    """Reloads the object from the database.
    This grabs the most recent version of the object from the database and
    updates the document accordingly. The data will change if the object
    from the database has been changed at one point.
    This only works if the object has been saved at least once before.
    Returns:
      self for OOP.
    Raises:
      NotFoundError: if the object hasn't been saved before.
    """
    if self._obj:
      self._obj.reload(r=r, vtag=vtag)
      if not self._obj.exists():
        self._deleted()
      else:
        self.saved = True
        self.deleted = False
        self.deserialize(self._obj.get_data())
        self.setIndexes(self._getIndexesFromRiakObj(self._obj))
        self.setLinks(self._getLinksFromRiakObj(self._obj))
    else:
      raise NotFoundError("Object not saved!")

  def _deleteBackRef(self, col_name, docs):
    docs_to_be_saved = []
    for doc in docs:
      if doc._meta[col_name].deleteReference(doc, self):
        docs_to_be_saved.append(doc)

    return docs_to_be_saved

  def delete(self, rw=None):
    """Deletes this object from the database. Same interface as riak-python.
    However, this object can still be resaved. Not sure what you would do
    with it, though.
    """

    if self._obj is not None:
      docs_to_be_saved = []
      for k in self._meta:
        # is_reference_back is for deleting the document that has the collection_name
        # collection_name is the document that gives out collection_name
        col_name = getattr(self._meta[k], "is_reference_back", False) or getattr(self._meta[k], "collection_name", False)

        if col_name:
          docs = getattr(self, k, [])
          if docs is not None:
            if isinstance(docs, Document):
              docs = [docs]
            docs_to_be_saved.extend(self._deleteBackRef(col_name, docs))

      self.__class__.instances.pop(self.key, False)

      self._obj.delete(rw=rw)

      for name in self._uniques:
        if self._data[name] is not None:
          obj = self._meta[name].unique_bucket.get(self._data[name])
          obj.delete()

      self._deleted()

      for doc in docs_to_be_saved:
        doc.save()

  def _deleted(self):
    self._obj = None
    self.saved = False
    self.deleted = True
    self.clear(False)

  def links(self, riakLinks=False):
    """Gets all the links.
    Args:
      riakLinks: Defaults to False. If True, it will return a list of RiakLinks
    Returns:
      A set of (document, tag) or [RiakLink, RiakLink]"""
    if riakLinks:
      return [RiakLink(self.bucket_name[0], d.key, t) for d, t in self._links]
    return copy(self._links)

  def getRawData(self, name, default=DocumentMetaclass):
    """Gets the raw data that's contained in the RiakObject.
    If default is not specified, AttributeError will be raised if the attribute
    doesn't exist
    If the object is not saved. NotFoundError will be raised
    Args:
      name: The name of the attribute.
      default: The default to return if not available. Defaults to some garbage, which is DocumentMetaclass
    Returns:
      The value or default.
    Raises:
      AttributeError if default is not specified and attribute not found
      NotFoundError if default not specified and object not found.
    """

    if self._obj:
      data = self._obj.get_data()
      if default == DocumentMetaclass:
        if name not in data:
          self._attrError(name)
        else:
          return data[name]
      else:
        return data.get(name, default)
    else:
      if default == DocumentMetaclass:
        raise NotFoundError("%s is not loaded!" % self.key)
      else:
        return default


  @staticmethod
  def _getLinksFromRiakObj(robj):
    objLinks = robj.get_links()
    links = set()
    for link in objLinks:
      tag = link.get_tag()
      c = getClassGivenBucketName(link.get_bucket())
      links.add((c.load(link.get(), True), tag))
    return links

  @classmethod
  def load(cls, robj, cached=False, r=None, bucket=None):
    """Construct a Document based object given a RiakObject.
    Args:
      riak_obj: The RiakObject that the document is suppose to build from.
      cached: Reload the object or not if it's found in the pool of objects.
      r: R value
      bucket: The bucket to grab from. Defaults to the default bucket.
    Returns:
      A Document object (whichever subclass this was called from).
    """

    if isinstance(robj, RiakObject):
      key = robj.get_key()
    else:
      key = robj

    try:
      doc = cls.instances[key]
    except KeyError:
      bucket = cls.buckets.get(bucket, cls.bucket)
      robj = bucket.get(key, r)
      if not robj.exists():
        raise NotFoundError("%s not found!" % key)

      # This is done before so that deserialize won't recurse
      # infinitely with collection_name. This wouldn't cause an problem as
      # deserialize calls for the loading of the referenced document
      # from cache, which load this document from cache, and it see that it
      # exists, finish loading the referenced document, then come back and finish
      # loading this document.

      doc = cls(key, saved=True)
      doc._obj = robj
      cls.instances[key] = doc
      doc.reload()
    else:
      if not cached:
        doc.reload()

    return doc

  @classmethod
  def get(cls, key, cached=True, r=None, bucket=None):
    """Same as load, but the default of the cached is True.
    This method is usually used and usually you just need a cached copy if
    available."""
    return cls.load(key, cached, r, bucket)

  @classmethod
  def getOrNew(cls, key, cached=True, r=None, bucket=None, **kwargs):
    """Similar to get, but does not raise error if not found. A new (unsaved)
    document will be created.
    Args:
      Everything: The same as get.
      For this method. The bucket argument only has effect on the get operation.
      **kwargs: Additional kwargs to merge data into the document.
    """
    try:
      d = cls.load(key, cached, r, bucket)
      d.mergeData(kwargs)
      return d
    except NotFoundError:
      return cls(key=key, **kwargs)

  @classmethod
  def exists(cls, key, r=None, bucket=None):
    """Check if a key exists.
    Args:
      key: The key to check if exists or not.
      r: The R value
    Returns:
      True if the key exists, false otherwise.
    """
    return cls.buckets.get(bucket, cls.bucket).get(key, r).exists()

  @classmethod
  def search(cls, querytext, bucket=None):
    """Searches through the bucket with some query text.
    The bucket must have search installed via search-cmd install BUCKETNAME. The
    class must have been marked to be  with cls. = True.
    Args:
      querytext: The query text as outlined in the python-riak documentations.
      bucket: The bucket to search. Leave default for the default bucket.
    Returns:
      A MapReduceQuery object. Similar to the RiakMapReduce object."""
    query_obj = cls.client.search(cls.bucket_name[0] if bucket is None else bucket, querytext)
    return MapReduceQuery(cls, query_obj)

  @classmethod
  def solrSearch(cls, querytext, bucket=None, **kwargs):
    """Searches through using the SOLR.
    Args:
      querytext: The query text
      kwargs: Any other keyword arguments for SOLR.
      bucket: The bucket to SOLR. Leave default for the default bucket.
    Returns:
      A SolrQuery object. Similart to a MapReduceQuery"""
    return SolrQuery(cls, cls.client.solr().search(cls.bucket_name[0] if bucket is None else bucket, querytext, **kwargs))

  @classmethod
  def indexLookup(cls, index, startkey, endkey=None, bucket=None):
    """Short hand for creating a new mapreduce index
    Args:
      index: The index field
      startkey: The starting key
      endkey: The ending key. If not none, search a range. Default: None
      bucket: The bucket to index. Leave default for the default bucket.
    Returns:
      A MapReduceQuery object
    """
    return MapReduceQuery(cls, cls.client.index(cls.bucket_name[0] if bucket is None else bucket, index, startkey, endkey))

  @classmethod
  def mapreduce(cls, bucket=None): # TODO: Make a better interface
    """Shorthand for creating a query object for map reduce.
    Returns:
      A RiakMapReduce object.
    """
    return cls.client.add(cls.bucket_name[0] if bucket is None else bucket)

    import re
from riakkit.commons import rndstr

_emailRegex = re.compile("[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*@(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?", flags=re.I)

# Credit goes to dictshield at https://github.com/j2labs/dictshield/blob/master/dictshield/fields/base.py
_urlRegex = re.compile(
    r'^https?://'
    r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+[A-Z]{2,6}\.?|'
    r'localhost|'
    r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})'
    r'(?::\d+)?'
    r'(?:/?|[/?]\S+)$', re.IGNORECASE
  )

_regexMatch = lambda x, r: True if x is None else bool(r.match(x.strip().lower()))

# Validators
emailValidator = lambda x: _regexMatch(x, _emailRegex) or x == ""
urlValidator = lambda x: _regexMatch(x, _urlRegex) or x == ""

# Password stuff
try:
  import bcrypt
except ImportError:
  import sys
  import string
  import random
  from hashlib import sha256

  print >> sys.stderr, "========================================================================"
  print >> sys.stderr, "WARNING: BCRYPT NOT AVAILABLE. INSTALL PY-BCRYPT FOR ADDITIONAL SECURITY"
  print >> sys.stderr, "========================================================================"

  generateSalt = lambda: rndstr(25)
  hashPassword = lambda password, salt: sha256(password + salt).hexdigest()
else:
  generateSalt = lambda: bcrypt.gensalt(10)
  hashPassword = lambda password, salt: bcrypt.hashpw(password, salt)

checkPassword = lambda password, passwordInDb: hashPassword(password, passwordInDb.salt) == passwordInDb.hash

class SolrQuery(object):
  """A wrapper around RiakSearch to play nice with Document and Solr
  Attributes:
    cls: The class for this SolrQuery
    result: The result dictionary.
  """
  def __init__(self, cls, result):
    self.cls = cls
    self.result = result
    self.loadDoc = lambda doc : self.cls.load(self.cls.bucket.get(doc[u"id"]))

  def length(self):
    """Gets the length of the documents that's searched through.
    Deprecated. Use len() instead."""
    return self.result[u"num_found"]

  __len__ = length

  def run(self):
    """Returns a generator that goes through each document that's searched."""
    for doc in self.result[u"docs"]:
      yield self.loadDoc(doc)

  def all(self):
    """Returns all the items that's found and return it.
    Return:
      A list of all the Documents.
    """
    return map(self.loadDoc, self.result[u"docs"])


class MapReduceQuery(object):
  """A wrapper around RiakMapReduce to play nice with Document
  Attributes:
    cls: The class for this MapReduceQuery.
    mr_obj: The original RiakMapReduce object.
    riak_links: All the links returned from the run operation of RiakMapReduce.
  """
  def __init__(self, cls, mr_obj):
    self.cls = cls
    self.mr_obj = mr_obj
    self.riak_links = mr_obj.run()

  def run(self):
    """A generator that goes through riak_link"""
    for link in self.riak_links:
      yield self.cls.load(link.get())

  def length(self):
    """The number of objects in this query.
    Deprecated. Use len() instead.
    Return:
      an integer that is the length of riak_obj
    """
    return len(self.riak_links)

  __len__ = length

  def all(self):
    """Returns all the Documents in a single list.
    Returns:
      A list containing all the Documents
    """
    return map(lambda link: self.cls.load(link.get()), self.riak_links)

    
from riakkit.commons import walkParents, uuid1Key
from riakkit.commons.properties import BaseProperty, ReferenceBaseProperty
from riakkit.commons.exceptions import ValidationError

from copy import copy, deepcopy
import json
from riak.mapreduce import RiakLink

class BaseDocumentMetaclass(type):
  def __new__(cls, clsname, parents, attrs):
    if clsname in ("BaseDocument", "SimpleDocument"):
      return type.__new__(cls, clsname, parents, attrs)

    meta = {}
    for name in attrs.keys():
      if isinstance(attrs[name], BaseProperty):
        meta[name] = attrs.pop(name)

    all_parents = reversed(walkParents(parents, ("BaseDocument", "SimpleDocument", "object", "type")))

    for p_cls in all_parents:
      meta.update(copy(p_cls._meta))
    attrs["_meta"] = meta

    return type.__new__(cls, clsname, parents, attrs)

  def __getattr__(self, name):
    if hasattr(self, "_meta") and name in self._meta:
      return self._meta[name]
    raise AttributeError("'%s' does not exist for class '%s'." % (name, self.__name__))

DEFAULT_VALIDATOR = lambda x: True
DEFAULT_CONVERTER = lambda x: x

class BaseDocument(object):
  """The BaseDocument class is the lowest level of abstraction ther is. This
  is essentially what dictshield has, probably even simpler (having never used
  dictshield in the first place).

  It's recommended that this is not used as SimpleDocument and Document is
  prefered.

  EmDocument is currently defined as EmDocument = BaseDocument. However, this
  behaviour may change in the future.
  """
  __metaclass__ = BaseDocumentMetaclass

  # This is not a real object. Objects created with this class will not have key
  # Either embedded document or otherwise.
  # TODO: Make this better. Potentially using some sort of detection system.
  # Or make a subclass of this that will be used as people abandon the overhead
  # of the RAD and use the core for efficiency.
  _clsType = 0

  def __init__(self, **kwargs):
    """Initialize a new BaseDocument.

    Args:
      kwargs: The keyword arguments to merge into the object.
    """
    self.clear()

    # TODO: combine with mergeData
    keys = set(self._meta.keys())
    for name, value in kwargs.iteritems():
      self.__setattr__(name, value)
      keys.discard(name)

    for name in keys:
      self._data[name] = self._meta[name].defaultValue()

  def _attrError(self, name):
    raise AttributeError("Attribute %s not found with %s." %
        (name, self.__class__.__name__))

  def _valiError(self, value, name):
    raise ValidationError(name,
        "%s doesn't pass validation for property '%s'" % (value, name)
    )

  def serialize(self, dictionary=True):
    """Serializes the data to database friendly version.

    This *only* returns the dictionary of values.

    Args:
      dictionary: If True, this function will return a dictionary passed back
                  to riak-python-client. Otherwise it will return a JSON.
    Returns:
      A dictionary or a string. Depending on the value of dictionary.
    """
    d = {}
    for name, value in self._data.iteritems():
      self._processOneValue(d, name, value)

    if dictionary:
      return d
    else:
      return json.dumps(d)

  def _processOneValue(self, d, name, value):
    prop = self._meta.get(name, None)
    converter = DEFAULT_CONVERTER
    if prop is not None:
      converter = prop.convertToDb

    if not self.validate(name):
      self._valiError(value, name)

    value = converter(value)

    d[unicode(name)] = value

  def valid(self):
    """Validate all the values.

    Returns:
      True if validation passes, False otherwise
    """
    for name, prop in self._meta.iteritems():
      if not self.validate(name):
        return False

    return True

  def validate(self, name):
    """Validate a specific property.

    Args:
      name: The name of the property. If it's not defined in the schema, this
            method will always return True

    Returns:
      True if valid, False otherwise.
    """
    if name in self._meta:
      prop = self._meta[name]
      value = self._data[name]
      if prop.required and value is None:
        return False
      else:
        return prop.validate(value)
    return True

  @classmethod
  def constructObject(cls, data):
    """Construct an object given some data.

    This is basically deserialize but as a classmethod.
    Args:
      data: Same thing as deserialize
      dictionary: Same thing as deserialize
    Returns:
      The object constructed with this class
    """
    return cls().deserialize(data)

  def deserialize(self, data):
    """Deserializes some data into the document.

    With this function, we assume the data is from the database, therefore we
    call convertFromDb. This method will also clear the document.

    Args:
      data: The data, either a dictionary or a json string.

    Returns:
      self for OOP purposes.
    """
    if isinstance(data, basestring):
      data = json.loads(data)

    self.clear()
    keys = set(self._meta.keys())
    for name, value in data.iteritems():
      prop = self._meta.get(name, None)
      if prop is not None:
        converter = prop.convertFromDb
      else:
        converter = DEFAULT_CONVERTER

      value = converter(value)
      self._data[name] = value
      keys.discard(name)

    for name in keys:
      self._data[name] = self._meta[name].defaultValue()

    return self

  def mergeData(self, data):
    """Merges some data into the document.

    With this function, we assume the data is from just the input source,
    therefore we call standardize. This method will NOT clear the document.

    Args:
      data: The data, either a dictionary of a json string.

    Returns:
      self for OOP"""

    if isinstance(data, basestring):
      data = json.loads(data)

    for name, value in data.iteritems():
      self.__setattr__(name, value)

    return self


  def clear(self, setdefault=True):
    """Clears the document, clears all the data stored.

    Returns:
      self for OOP"""
    self._data = {}

    if setdefault:
      for name, prop in self._meta.iteritems():
        self._data[name] = prop.defaultValue()

    return self

  def __setattr__(self, name, value):
    if name.startswith("_"):
      self.__dict__[name] = value
      return

    validator = lambda x: True
    standardizer = lambda x: x
    if name in self._meta:
      validator = self._meta[name].validate
      standardizer = self._meta[name].standardize

    if not validator(value):
      self._valiError(value, name)

    value = standardizer(value)
    self._data[name] = value

  def __getattr__(self, name):
    if name in self._data:
      prop = self._meta.get(name, BaseProperty)
      if isinstance(prop, ReferenceBaseProperty):
        self._data[name] = prop.attemptLoad(self._data[name])
      return self._data[name]

    self._attrError(name)

  def __delattr__(self, name):
    if name in self._data:
      if name in self._meta:
        self._data[name] = None
      else:
        del self._data[name]
    else:
      raise KeyError("'%s'" % name)

  __setitem__ = __setattr__
  __getitem__ = __getattr__
  __delitem__ = __delattr__


class SimpleDocument(BaseDocument):
  """This is a low level abstract of how objects that would be directly saved
  into Riak (ones associated with a key, not an embedded document).

  This class provides nearly all operations that Document provides, except
  the ones that's interacts directly with Riak.

  unique no longer has any meanings here as it's SimpleDocument's job to enforce
  this rule. That's your problem.
  """
  _clsType = 1

  def __init__(self, key=uuid1Key, **kwargs):
    """Creates a SimpleDocument object.

    Args:
      key: A key string or a callable. Defaults to generating uuid1
      **kwargs: Any data that you want to be merged into the document
    """
    if callable(key):
      key = key(kwargs)

    self.__dict__["key"] = key

    BaseDocument.__init__(self, **kwargs)

  def clear(self, setdefault=True):
    self._indexes = {}
    self._links = set()
    return BaseDocument.clear(self, setdefault)

  def save(self, **kwargs):
    """Not available in SimpleDocument.

    raises:
      NotImplementedError
    """
    raise NotImplementedError("Remember the good old day when we played with Documents? Well as an adult now, you don't save SimpleDocuments anymore.")

  reload = save
  delete = save

  def addIndex(self, field, value):
    """Adds an index field : value

    Args:
      field: The field name. Remember that fieldnames have to end with the
             correct types, and currently it's _bin and _int.
      value: The value for this field.

    Returns:
      self for OOP purposes.
    """
    l = self._indexes.get(field, set())
    l.add(value)
    self._indexes[field] = l
    return self

  def removeIndex(self, field, value=None, silent=False):
    """Removes an index field : value

    Args:
      field: The field name.
      value: The value, defaults to None. If value is none, the whole field gets
             removed, otherwise only that specific pair will get removed.
      slient: If set to True and field doesn't exist, it will not raise a KeyError,
              otherwise it will.
    Returns:
      self for OOP purposes
    """
    if value is None:
      if silent:
        self._indexes.pop(field, None)
      else:
        self._indexes.pop(field)
    else:
      if field in self._indexes:
        self._indexes[field].discard(value)
        if len(self._indexes[field]) == 0:
          self._indexes.pop(field)

    return self

  def setIndexes(self, indexes):
    """Sets the indexes. Overwrites the previous indexes

    Args:
      indexes: Format should be {"fieldname" : {"fieldvalue"}, "fieldname2" : {"fieldvalue"}}.
               A deep copy is made here.

    Returns:
      self for OOP purposes.
    """
    self._indexes = deepcopy(indexes)
    return self

  def indexes(self, field=None, default=BaseDocumentMetaclass):
    """Retrives the whole index or a specific list of indexes for a field.

    Args:
      field: the field name. Defaults to None. If it is None, the all the indexes will be returned.
      default: The default value to return. If left as BaseDocumentMetaclass, KeyError will be raised.
               Otherwise it will return default instead of raising a KeyError.

    Returns:
      The set of field values or a list of (field, value) pairs friendly for set_indexes
    """
    if field is not None:
      try:
        return copy(self._indexes[field])
      except KeyError, e:
        if default is BaseDocumentMetaclass:
          raise e
        else:
          return default

    i = []
    for field, l in self._indexes.iteritems():
      for value in l:
        i.append((field, value))

    return i

  index = indexes # Done so that index(fieldname) is grammatically correct

  def addLink(self, document, tag=None):
    """Adds a link for the document.

    Args:
      document: A SimpleDocument object or its child. Checking will not be done.
      tag: A tag. Defaults to None

    Returns:
      self for OOP purposes"""
    self._links.add((document, tag))
    return self

  def removeLink(self, document, tag=None):
    """Removes a link from the document

    Args:
      document: A SimpleDocument object or its child.
      tag: A tag value.

    Returns:
      self for OOP purposes"""
    l = set()
    for d, t in self._links:
      if d.key != document.key or tag != t: # TODO: Best way to do this?
        l.add((d, t))
    self._links = l
    return self

  def setLinks(self, links):
    """Sets the links. Overwrites the current links collection.

    Args:
      links: Format should be set((document, tag), (document, tag)).
             A shallow copy is made here.

    Returns:
      self for OOP purposes"""
    self._links = copy(links)
    return self

  def links(self, bucket=None):
    """Gets all the links.

    Args:
      bucket: Defaults to None. If it is a RiakBucket, this will return a list of RiakLinks instead of (document, tag) in a set

    Returns:
      A set of (document, tag) or [RiakLink, RiakLink]"""
    if bucket is not None:
      return [RiakLink(bucket.get_name(), d.key, t) for d, t in self._links]
    return copy(self._links)

  def toRiakObject(self, bucket):
    """Converts the SimpleDocument into a RiakObject. Does not touch references,
    unlike Document.save(). Nor does this actually save anything.

    Args:
      bucket: A RiakBucket object

    Returns:
      A RiakObject with data, indexes, and links set according to this
      SimpleDocument
    """
    obj = bucket.new(self.key, self.serialize())
    obj.set_indexes(self.indexes())
    obj.set_links(self.links(bucket), True)
    return obj

  @staticmethod
  def _getIndexesFromRiakObj(robj):
    objIndexes = robj.get_indexes()
    indexes = {}
    for indexEntry in objIndexes:
      field = indexEntry.get_field()
      value = indexEntry.get_value()
      l = indexes.get(field, set())
      l.add(value)
      indexes[field] = l
    return indexes

  @classmethod
  def load(cls, robj, cached=False):
    """Construct a SimpleDocument from a RiakObject. Similar to Document.load,
    but doesn't support links, nor does it load from the database via a key
    instead of a robj, nor does it do caching.

    Note that this function does not get the links as SimpleDocument can't do
    this due to technical reason (one way only).

    Args:
      robj: A RiakObject

    Returns:
      A SimpleDocument
    """
    doc = cls(robj.get_key())
    doc.deserialize(robj.get_data())
    doc.setIndexes(cls._getIndexesFromRiakObj(robj))
    return doc

    from __future__ import with_statement

from copy import copy
from functools import partial
import operator

from django.contrib import admin
from django.contrib.admin import helpers
from django.contrib.admin.util import unquote
from django.core.exceptions import PermissionDenied, ValidationError, FieldError
from django.core.paginator import Paginator, EmptyPage, InvalidPage
from django.core.urlresolvers import reverse, NoReverseMatch
from django.db import models, transaction
from django.forms.formsets import all_valid
from django.http import HttpResponseBadRequest, HttpResponseServerError, Http404
from django.shortcuts import get_object_or_404, redirect, render_to_response
from django.template import RequestContext
from django.utils.encoding import force_unicode
from django.forms.util import ErrorList
from django.utils.functional import update_wrapper
from django.utils.html import escape
from django.utils.safestring import mark_safe
from django.utils.translation import ugettext_lazy as _

from forms import SectionForm
import app_settings

app_name =  app_settings.EXTENDING_APP_NAME
allow_associated_ordering = app_settings.ALLOW_ASSOCIATED_ORDERING
model_proxy = app_settings.get_extending_model()

csrf_protect_m = admin.options.csrf_protect_m


class SectionAdmin(admin.ModelAdmin):

    class Media:
        css = {
            "all": ("scaffold/styles/scaffold-admin.css",)
        }

    form = SectionForm
    list_per_page = 10
    template_base = "scaffold/admin/"
    prepopulated_fields = {"slug": ("title",)}

    change_form_template ="scaffold/admin/change_form.html"

    def get_urls(self):

        try:
            from django.conf.urls import patterns, url
        except ImportError:
            from django.conf.urls.defaults import patterns, url

        def wrap(view):
            def wrapper(*args, **kwargs):
                return self.admin_site.admin_view(view)(*args, **kwargs)
            return update_wrapper(wrapper, view)

        urls = super(SectionAdmin, self).get_urls()
        info = self.model._meta.app_label, self.model._meta.module_name
        return patterns('',
            url(r'^(.+)/create/$',
                wrap(self.custom_add_view),
                name='%s_%s_create' % info),
            url(r'^(.+)/move/$',
                wrap(self.move_view),
                name='%s_%s_move' % info),
            url(r'^(.+)/related/$',
                wrap(self.related_content_view),
                name='%s_%s_related' % info),
            url(r'^(.+)/order/$',
                wrap(self.order_content_view),
                name='%s_%s_order' % info),
        ) + urls

    def has_view_permission(self, request):
        """
        Returns True if the given request has permission to view the given
        Django model instance.

        If `obj` is None, this should return True if the given request has
        permission to change *any* object of the given type.
        """
        opts = self.opts
        return request.user.has_perm((
            opts.app_label + '.can_view_associated_content'
        ))

    @property
    def app_context(self):
        """
        Returns a dictionary containing the app name, model name, plural model
        name and changelist (index) url.
        """
        meta = self.model._meta
        return {
            'app_index_url': reverse('admin:app_list', args=(meta.app_label,)),
            'app_label': meta.app_label,
            'module_label': meta.module_name,
            'model_label': meta.verbose_name,
            'model_label_plural': unicode(meta.verbose_name_plural),
            'changelist_url': reverse('admin:%s_%s_changelist' %
            (meta.app_label, meta.module_name)),
        }

    def render_scaffold_page(self, request, template, context):
        """
        Helper function to render a scaffold admin page.
        """
        context.update(self.app_context)
        template_path = self.template_base + template
        return render_to_response(template_path, context, context_instance=RequestContext(request))

    def redirect_to_scaffold_index(self, request):
        """Redirect to the change list page of your model."""
        redirect_url = reverse(
            'admin:%(app_label)s_%(module_label)s_changelist'
            % self.app_context
        )
        return redirect(redirect_url, permanent=False)

    def redirect_to_object_changeform(self, request, obj):
        """Redirect to the change form of the given object."""
        redirect_url = reverse(
            'admin:%(app_label)s_%(module_label)s_change' % self.app_context,
             args=(obj.pk,)
        )
        return redirect(redirect_url, permanent=False)

    def get_changelist_repr(self, node):
        """
        A method that takes a node in the tree and returns a string
        representation for the changelist view.
        """
        html = '<span><a href="%s">%s <small> &ndash; /%s/</small></a></span>'
        return html % (node.get_absolute_url(), node.title, node.full_path)

    def changelist_view(self, request):
        """
        Display a tree of section and subsection nodes.
        Because of the impossibility of expressing needed concepts (e.g.
        recursion) within the django template syntax, the tree html (nested
        <ul> elements) is constructed manually in this view using the crawl
        function.
        """
        model = self.model

        if not self.has_view_permission(request):
            raise PermissionDenied

        roots = model.get_root_nodes()
        link_html = _get_user_link_html(request)
        link_html_fields = [name for name, html in link_html]
        link_html_dict = dict(link_html)

        def crawl(node, admin_links=[]):
            """
            Nests a series of treebeard nodes in unordered lists
            """
            # Generate HTML for the current node
            html = (
                '<li id="node-%s">%s<div class="links">%s</div>'
            ) % (
                node.id,
                self.get_changelist_repr(node),
                " ".join([link_html_dict[l] % node.pk for l in admin_links])
            )
            # Inject submenu of children, if applicable
            if not node.is_leaf():
                children = node.get_children()
                children = "".join(
                    map(partial(crawl, admin_links=admin_links), children)
                )
                html += "<ul>%s</ul>" % children

            return html + "</li>"

        crawl_add_links = partial(
            crawl,
            admin_links=link_html_fields
        )

        # Generate HTML
        node_list_html = '<ul id="node-list">'
        node_list_html += "".join(map(crawl_add_links, roots))
        node_list_html += "</ul>"

        context = {
            'node_list':node_list_html,
            'title': "Edit %s" % self.app_context['model_label_plural']
        }
        return self.render_scaffold_page(request, 'index.html', context)

    def add_view(self, request):
        """
        This view will not be used because adding of nodes to the tree can
        never be done without context (i.e. where in the tree the new node is
        to be positioned). Therefore, the "add" links that appear on the index
        of the admin site will not work, hence this redirect to the model
        changeform when it's clicked.
        """
        return self.redirect_to_scaffold_index(request)

    def custom_add_view(self, request, section_id,
        form_url='', extra_context=None):
        """
        The 'add' admin view for this model.

        We're managing transactions manually on this one because of the way
        treebeard works. Treebeard wraps a model's save method and doesn't allow
        us to pass the `commit=False` argument to the save method. But the
        admin wants to do this because creation of the model depends on also
        being able validate and save any inlines. Treebeard does however use the
        `transaction.commit_unless_managed()` feature in it's save wrappers.

        We can exploit this to safely unwind the creation of all objects if
        something fails along the way.

        """
        model = self.model
        opts = model._meta

        if not self.has_add_permission(request):
            raise PermissionDenied
        if section_id == 'root':
            parent = None
        else:
            parent = model.objects.get(pk=section_id)
            setattr(parent, 'has_children', len(parent.get_children()) > 0)

        ModelForm = self.get_form(request)
        formsets = []
        if request.method == 'POST':
            with transaction.commit_manually():
                # Becuase transactions are managed manually, any DB
                # or validation operation that occurs durint the processing of a
                # POST request will raise a ValidationError exception if it
                # encounters a problem. The giant try/catch block which wraps
                # all this allows us to rollback if any problems occur.
                try:
                    form = ModelForm(request.POST, request.FILES)
                    if form.is_valid():
                        try:
                            kwargs = form.cleaned_data
                            new_object = self.validate_and_create_object(
                                parent,
                                kwargs
                            )
                            form_validated = True
                        except ValidationError, e:
                            # Validation can't happen via the usual channels, so
                            # we're going to monkey with the form's error list.
                            form_validated = False
                            form._errors['slug'] = ErrorList()
                            form._errors['slug'].append(e.messages[0])
                            new_object = self.model()
                        except Exception, e:
                            # Something bad has happened; not sure what, so we'll
                            # punt by re-raising as a FieldError which will be
                            # handled by the outer try/catch.
                            raise FieldError, e
                    else:
                        form_validated = False
                        new_object = self.model()
                    prefixes = {}
                    inline_instances = self.get_inline_instances(request)
                    for FormSet, inline in\
                        zip(self.get_formsets(request), inline_instances):
                        prefix = FormSet.get_default_prefix()
                        prefixes[prefix] = prefixes.get(prefix, 0) + 1
                        if prefixes[prefix] != 1:
                            prefix = "%s-%s" % (prefix, prefixes[prefix])
                        formset = FormSet(
                            data=request.POST,
                            files=request.FILES,
                            instance=new_object,
                            save_as_new=request.POST.has_key("_saveasnew"),
                            prefix=prefix,
                            queryset=inline.queryset(request)
                        )
                        formsets.append(formset)
                    if all_valid(formsets) and form_validated:
                        # The object validated and saved, the inlines appear to
                        # be valid, now we will save them one by one:
                        try:
                            for formset in formsets:
                                self.save_formset(request, form, formset,
                                    change=False
                                )
                        except Exception, e:
                            raise ValidationError, e
                        if form_validated and request.POST.get('position') \
                            and request.POST.get('child'):
                            try:
                                rel_to = model.objects.get(
                                    pk=request.POST.get('child')
                                )
                            except model.DoesNotExist, e:
                                raise ValidationError, e
                            rel = request.POST.get('position')
                            pos_map = {
                                'before': 'left',
                                'after': 'right'
                            }
                            if rel not in pos_map.keys():
                                positions = ", ".join(pos_map.keys())
                                raise ValidationError, (
                                    "Position must be one of: %s" %
                                    positions
                                )
                            try:
                                new_object.move(rel_to, pos_map[rel])
                            except Exception, e:
                                e = str(e)
                                raise ValidationError, "Unable to move: %s" % e

                        self.log_addition(request, new_object)
                        transaction.commit()
                        if request.POST.has_key("_continue"):
                            return self.redirect_to_object_changeform(
                                request,
                                new_object
                            )
                        return self.redirect_to_scaffold_index(request)
                    else:
                        # Fieldset validation error, so we rollback
                        transaction.rollback()
                except ValidationError, e:
                        transaction.rollback()
                        return HttpResponseBadRequest(" ".join(e.messages))
        else:         # Request is not POST

            # Prepare the dict of initial data from the request.
            # We have to special-case M2Ms as a list of comma-separated PKs.
            initial = dict(request.GET.items())
            for k in initial:
                try:
                    f = opts.get_field(k)
                except models.FieldDoesNotExist:
                    continue
                if isinstance(f, models.ManyToManyField):
                    initial[k] = initial[k].split(",")
            form = ModelForm(initial=initial)
            prefixes = {}
            inline_instances = self.get_inline_instances(request)
            for FormSet, inline in zip(self.get_formsets(request), inline_instances):
                prefix = FormSet.get_default_prefix()
                prefixes[prefix] = prefixes.get(prefix, 0) + 1
                if prefixes[prefix] != 1:
                    prefix = "%s-%s" % (prefix, prefixes[prefix])
                formset = FormSet(instance=self.model(), prefix=prefix,
                                  queryset=inline.queryset(request))
                formsets.append(formset)

        adminForm = helpers.AdminForm(form, list(self.get_fieldsets(request)),
            self.prepopulated_fields, self.get_readonly_fields(request),
            model_admin=self)
        media = self.media + adminForm.media

        inline_admin_formsets = []
        inline_instances = self.get_inline_instances(request)
        for inline, formset in zip(inline_instances, formsets):
            fieldsets = list(inline.get_fieldsets(request))
            readonly = list(inline.get_readonly_fields(request))
            inline_admin_formset = helpers.InlineAdminFormSet(inline, formset,
                fieldsets, readonly_fields=readonly, model_admin=self)
            inline_admin_formsets.append(inline_admin_formset)
            media = media + inline_admin_formset.media

        context = {
            'add': True,
            'parent': parent,
            'title': _('Add %s') % force_unicode(opts.verbose_name),
            'adminform': adminForm,
            'is_popup': request.REQUEST.has_key('_popup'),
            'show_delete': False,
            'media': mark_safe(media),
            'inline_admin_formsets': inline_admin_formsets,
            'errors': helpers.AdminErrorList(form, formsets),
            'app_label': opts.app_label,
        }
        context.update(extra_context or {})
        return self.render_scaffold_page(request, "add.html", context)

    @csrf_protect_m
    @transaction.commit_on_success
    def delete_view(self, request, object_id):
        """
        This view allows the user to delete Sections within the node tree.
        """
        model = self.model

        try:
            obj = model.objects.get(pk=object_id)
        except model.DoesNotExist:
            # Don't raise Http404 just yet, because we haven't checked
            # permissions yet. We don't want an unauthenticated user to be able
            # to determine whether a given object exists.
            obj = None
        if not self.has_delete_permission(request, obj):
            raise PermissionDenied
        if not obj:
            raise
        if request.method == 'POST':
            obj.delete()
            # Log that a section has been successfully deleted.
            self.log_deletion(request, obj, obj.title)
            return self.redirect_to_scaffold_index(request)
        context = {
            'obj': obj,
            'title': "Delete %s" % self.app_context['model_label']
        }
        return self.render_scaffold_page(request,
            "delete.html", context
        )
    delete_view = transaction.commit_on_success(delete_view)

    @csrf_protect_m
    @transaction.commit_on_success
    def move_view(self, request, object_id):
        """This view allows the user to move sections within the node tree."""
        #FIXME: should be an AJAX responder version of this view.

        model = self.model
        opts = model._meta

        try:
            obj = self.queryset(request).get(pk=unquote(object_id))
        except model.DoesNotExist:
            # Don't raise Http404 just yet, because we haven't checked
            # permissions. We don't want an unauthenticated user to be able
            # to determine whether a given object exists.
            obj = None
        if not self.has_change_permission(request, obj):
            raise PermissionDenied
        if obj is None:
            raise Http404(_(
                '%(name)s object with primary key %(key)r does not exist.') % {
                    'name': force_unicode(opts.verbose_name),
                    'key': escape(object_id)
            })

        if request.method == 'POST':
            rel = request.POST.get('relationship')
            if request.POST.get('to') == 'TOP':
                rel_to = obj.get_root_nodes()[0]
                rel = 'top'
            else:
                rel_to = get_object_or_404(model,
                    pk=request.POST.get('to')
                )
            if rel_to.pk == obj.pk:
                return HttpResponseBadRequest(
                "Unable to move node relative to itself."
                )
            pos_map = {
                'top': 'left',
                'neighbor': 'right',
                'child': 'first-child'
            }
            if rel not in pos_map.keys():
                return HttpResponseBadRequest(
                    "Position must be one of %s " % ", ".join(pos_map.keys())
                )
            try:
                obj.move(rel_to, pos_map[rel])
            except Exception, e:
                return HttpResponseServerError("Unable to move node. %s" % e)
            else:
                if model.find_problems()[4] != []:
                    model.fix_tree()
                # Log that a section has been successfully moved.
                change_message = "%s moved." % obj.title
                self.log_change(request, obj, change_message)
                # Redirect to sections index page.
                return self.redirect_to_scaffold_index(request)
        # Exclude the node from the list of candidates...
        other_secs = model.objects.exclude(pk=object_id)
        # ...then exclude descendants of the node being moved.
        other_secs = [n for n in other_secs if not n.is_descendant_of(obj)]

        # Provides a sections tree for user reference.
        def crawl(node):
            html_class = node.pk == obj.pk and ' class="active"' or ""
            if node.is_leaf():
                return "<li%s>%s</li>" % (html_class, node.title)
            else:
                children = node.get_children()
                html = "<li%s>%s<ul>" % (html_class, node.title)
                html += "".join(
                    map(crawl, children)
                )
                return html + "</ul></li>"
        root_nodes = self.model.get_root_nodes()
        tree_html = '<ul id="node-list" class="treeview-red">%s</ul>'
        tree_html = tree_html % ("".join(map(crawl, root_nodes)))
        context = {
            'obj': obj,
            'tree': other_secs,
            'title': "Move %s" % self.app_context['model_label'],
            'preview': tree_html
        }
        return self.render_scaffold_page(request,
            "move.html", context
        )
    move_view = transaction.commit_on_success(move_view)

    @csrf_protect_m
    @transaction.commit_on_success
    def change_view(self, request, object_id, extra_context=None):
        obj = self.get_object(request, unquote(object_id))
        if not obj:
            model_name = self.model._meta.module_name.title()
            raise Http404, "%s not found." % model_name
        rel_sort_key = allow_associated_ordering and 'order' or None
        context = {
            'allow_associated_ordering': allow_associated_ordering,
            'related_content': _get_content_table(obj, sort_key=rel_sort_key)
        }
        context.update(self.app_context)
        context.update(extra_context or {})
        return super(SectionAdmin, self).change_view(request, object_id,
            extra_context=context
        )

    def related_content_view(self, request, section_id, list_per_page=10):
        """
        This view shows all content associated with a particular section. The
        edit view also shows this info, but this view is for people who may not
        have permissions to edit sections but still need to see all content
        associated with a particular Section.
        """
        model = self.model

        try:
            obj = self.queryset(request).get(pk=unquote(section_id))
        except model.DoesNotExist:
            # Don't raise Http404 just yet, because we haven't checked
            # permissions. We don't want an unauthenticated user to be able
            # to determine whether a given object exists.
            obj = None
        if not self.has_view_permission(request):
            raise PermissionDenied
        content_table = _get_content_table(obj)
        sort = request.GET.get('sort')
        sort_map = {
            'name': 0,
            'date': 1,
            'content': 3
        }
        if sort and sort in sort_map.keys():
            content_table = sorted(
                content_table,
                key=operator.itemgetter(sort_map[sort])
            )
        paginated_content = Paginator(content_table, list_per_page)
        try:
            page = int(request.GET.get('page', '1'))
        except ValueError:
            page = 1
        # If page request is out of range, deliver last page of results.
        try:
            content_table = paginated_content.page(page)
        except (EmptyPage, InvalidPage):
            content_table = paginated_content.page(paginated_content.num_pages)
        context = {
            'obj': obj,
            'sort': sort,
            'related_content': content_table,
            'title': "Related %s content" % self.app_context['model_label'],
        }
        return self.render_scaffold_page(
            request,
            'related_content.html',
            context
        )

    @csrf_protect_m
    @transaction.commit_on_success
    def order_content_view(self, request, object_id):
        """
        This view shows all content associated with a particular section
        including subsections, but unlike related_content, this view allows
        users to set the order of a particular section.
        """
        model = self.model
        opts = model._meta

        try:
            obj = self.queryset(request).get(pk=unquote(object_id))
        except model.DoesNotExist:
            # Don't raise Http404 just yet, because we haven't checked
            # permissions. We don't want an unauthenticated user to be able
            # to determine whether a given object exists.
            obj = None
        if not self.has_change_permission(request, obj):
            raise PermissionDenied
        if obj is None:
            raise Http404(_(
                '%(name)s object with primary key %(key)r does not exist.') % {
                    'name': force_unicode(opts.verbose_name),
                    'key': escape(object_id)
            })
        if not app_settings.ALLOW_ASSOCIATED_ORDERING:
            return self.redirect_to_scaffold_index(request)
        content_table = _get_content_table(obj, sort_key='order')
        if request.method == 'POST':
            for item, date, app, model, rel, edit_url in content_table:
                item_id = "%s-%s-%s" % (app, model, str(item.pk))
                item_order = request.POST.get(item_id, None)
                if item_order and item_order.isdigit():
                    item.order = int(item_order)
                    item.save()
                else:
                    return HttpResponseBadRequest((
                        "Item order was not specified for every item, or the "
                        "order provided was not a number."
                    ))
            # Log that a section has been successfully edited.
            self.log_change(
                request,
                obj,
                "Content for %s reordered ." % obj.title
            )
            # Redirect to sections index page.
            return self.redirect_to_scaffold_index(request)
        context = {
            'obj': obj,
            'related_content': content_table,
            'title': "Order %s content" % self.app_context['model_label']
        }
        return self.render_scaffold_page(request, "order_all_content.html",
            context
        )

    def prep_m2m(self, kwargs):
        """
        Any kwargs which correspond to M2M fields on the model cannot be
        saved concurrently. This function removes those from the base kwargs
        and returns them separately so they can be added after the initial model
        is saved by treebeard.
        """
        m2m_rels = {}
        for field in self.model._meta.many_to_many:
            if field.name in kwargs:
                m2m_objects = kwargs.pop(field.name)
                m2m_rels[field.name] = m2m_objects
        return kwargs, m2m_rels

    def validate_and_create_object(self, parent, kwargs):
        """
        Based on the `VALIDATE_GLOBALLY_UNIQUE_SLUGS` setting, ensures that no
        other children with the slug exist *or* ensures that no other children
        at that level with the same slug exist. If that requirement is
        satisfied, creates and returns the object. Otherwise, a ValidationError
        is raised.

        This task, sadly, cannot be handled through traditional model or form
        validators because the model's save method is wrapped up in treebeard
        code.
        """
        slug = kwargs['slug']
        kwargs, m2m_data = self.prep_m2m(kwargs)
        verbose_name = self.model._meta.verbose_name
        if app_settings.VALIDATE_GLOBALLY_UNIQUE_SLUGS:
            try:
                self.model.objects.get(slug=slug)
            except self.model.DoesNotExist:
                if parent:
                    newobj = parent.add_child(**kwargs)
                else:
                    newobj = self.model.add_root(**kwargs)
                # Add any M2M related objects now.
                for field_name, related_objects in m2m_data.items():
                    m2m_manager = getattr(newobj, field_name)
                    m2m_manager.add(*related_objects)
                return newobj
            else:
                raise ValidationError, (
                    "A %s with the slug '%s' already exists"
                ) % (verbose_name, slug)
        # Validation if slugs do not have to be globally unique.
        else:
            if parent:
                if slug in [obj.slug for obj in parent.get_children()]:
                    err_str = (
                        "The %s '%s' already has a child with the "
                        "slug '%s.'"
                    ) % (verbose_name, parent.title, slug)
                    raise ValidationError, err_str
                else:
                    newobj = parent.add_child(**kwargs)
            else:
                if slug in [obj.slug for obj in \
                    self.model.get_root_nodes()]:
                    err_str = (
                        "A %s already exists at the root of the tree with "
                        "the slug %s."
                    ) % (verbose_name, slug)
                    verbose_name = self.model._meta.verbose_name
                    raise ValidationError, err_str
                else:
                    newobj = self.model.add_root(**kwargs)
            # Add any M2M related objects now.
            for field_name, related_objects in m2m_data.items():
                m2m_manager = getattr(newobj, field_name)
                m2m_manager.add(*related_objects)
            return newobj
        return None

######################################
#        Utility Functions
######################################


def _get_content_table(obj, sort_key=None):
    """
    Returns list of tuples containing:

    * the related object
    * its date (from get_latest_by prop, if it's set)
    * the application the object belongs to
    * the model the object belongs to
    * The type of relationship
    * the URL in the admin that will allow you to edit the object

    """
    related_content = obj.get_associated_content(sort_key=sort_key)
    content_table = []
    for item, app, model, relationship_type in related_content:
        edit_url = "admin:%s_%s_change" % (app, model.lower())
        try:
            edit_url = reverse(edit_url, args=[item.id])
        except NoReverseMatch:
            edit_url = None
        if item._meta.get_latest_by:
            date = getattr(item, item._meta.get_latest_by)
        else:
            date = None
        content_table.append((
             item,
             date,
             app,
             model,
             relationship_type,
             edit_url
        ))
    return content_table

def _get_user_link_html(request):
    """
    Checks available user permissions to make sure that the rendered changelist
    page does not offer the user options which they don't have permissions for
    (avoids having a PermissionDenied exception get raised).
    """
    link_html = copy(app_settings.LINK_HTML)
    app_label = model_proxy._meta.app_label
    model_proxy._meta.get_add_permission()
    add_perm = app_label + "." + model_proxy._meta.get_add_permission()
    del_perm = app_label + "." + model_proxy._meta.get_delete_permission()
    if not request.user.has_perm(add_perm):
        link_html = [(name, html) for name, html in  link_html.items() \
            if name != 'add_link'
        ]
    if not request.user.has_perm(del_perm):
        link_html = [(name, html) for name, html in  link_html.items() \
            if name != 'del_link'
        ]
    return link_html

    import re

from django.conf import settings
from django.core.exceptions import ImproperlyConfigured
from django.utils.importlib import import_module

_project_settings_registry = []

def _get_setting(project_setting_name, default=None, required=False):
    project_setting_name = "SCAFFOLD_%s" % project_setting_name
    _project_settings_registry.insert(0, project_setting_name)
    if required and not default:
            assert hasattr(settings, project_setting_name), (
                "The following setting is required to use the scaffold "                    
                "application in your project: %s"
            ) %  project_setting_name
    return getattr(settings, project_setting_name, default)

EXTENDING_APP_NAME = _get_setting('EXTENDING_APP_NAME', 
    required=True
)

EXTENDING_MODEL_PATH = _get_setting('EXTENDING_MODEL_PATH',     
    default = "%s.models.Section" % EXTENDING_APP_NAME
)

LINK_HTML = _get_setting('LINK_HTML', default=(
    ('edit_link', (
        "<a class=\"changelink\" href=\"%s/\">"
        "edit</a>"
    ),),
    ('add_link', (
        "<a class=\"addlink\" href=\"%s/create/\">"
        "add child</a>"
    ),),
    ('del_link', (
        "<a class=\"deletelink\" href=\"%s/delete/\">"
        "delete</a>" 
    ),),
    ('list_link', (
        "<a class=\"listlink\" href=\"%s/related/\">"
        "list content</a>" 
    ),)
))

PATH_CACHE_TTL = _get_setting('PATH_CACHE_TTL',
    default = (60 * 60 * 12)
)
PATH_CACHE_KEY = _get_setting('PATH_CACHE_KEY',
    default="scaffold-path-map" 
)

ALLOW_ASSOCIATED_ORDERING = _get_setting('ALLOW_ASSOCIATED_ORDERING',   
    default=True
)

VALIDATE_GLOBALLY_UNIQUE_SLUGS = _get_setting('VALIDATE_GLOBALLY_UNIQUE_SLUGS',   
    default=False
)

TREEBEARD_NODE_TYPE = _get_setting('TREEBEARD_NODE_TYPE',
    default="treebeard.mp_tree.MP_Node"
)

def get_extending_model():
    """
    This function returns the model that subclasses BaseSection.
    Since it's that real, non-abstract model we usually want to
    deal with, this function will be used extensively in views and
    middlewares. 
    """
    model_path = EXTENDING_MODEL_PATH.split(".")
    model_name = model_path.pop()
    submodule = model_path[-1]
    import_path = ".".join(model_path)
    models = __import__(import_path, fromlist=[submodule])
    return getattr(models, model_name)

def get_treebeard_node_class():
    """
    This function returns the Treebeard node type specified in the 
    ``TREEBEARD_NODE_TYPE`` value in this module or in the 
    ``SCAFFOLD_TREEBEARD_NODE_TYPE`` value in the main project settings
    file. Allowed values are:
        
        'treebeard.mp_tree.MP_Node'
        'treebeard.al_tree.AL_Node'
        'treebeard.ns_tree.NS_Node'
        
    Refer to the treebeard docs for an explanation of each type.
    
    """
    allowed_node_types = (
        'treebeard.mp_tree.MP_Node',
        'treebeard.al_tree.AL_Node',
        'treebeard.ns_tree.NS_Node',
    )
    if TREEBEARD_NODE_TYPE not in allowed_node_types:
        raise ImproperlyConfigured, (
            "The SCAFFOLD_TREEBEARD_NODE_TYPE setting must be one of the " 
            "following: %s."
        ) % ", ".join(allowed_node_types)
    try:
        klass_name = re.search('\.(\w*)$', TREEBEARD_NODE_TYPE).groups()[0]
    except AttributeError:
        raise ImproperlyConfigured, (
            "The SCAFFOLD_TREEBEARD_NODE_TYPE setting could not be parsed"
        )
    module_name = TREEBEARD_NODE_TYPE.replace("." + klass_name, '')
    try:
        module = import_module(module_name)
    except ImportError:
        raise ImproperlyConfigured, (
        "The module %s could not be imported. Please check your "
        "SCAFFOLD_TREEBEARD_NODE_TYPE setting."
        ) % module_name
    if not hasattr(module, klass_name):
        raise ImproperlyConfigured, (
        "The class %s could not be found in the module %s. Please check "
        "your SCAFFOLD_TREEBEARD_NODE_TYPE setting."
        ) % (klass_name, module_name)            
    return getattr(module, klass_name)